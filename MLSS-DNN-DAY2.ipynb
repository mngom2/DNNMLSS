{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/IntroLLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel and distributed Deep Learning\n",
    "\n",
    "## Author: Marieme Ngom, Argonne National Laboratory\n",
    "(combining and adapting materials/discussion evolved over time by Huihuo Zheng, Bethany Lusch, Asad Khan, Prasanna Balaprakash, Taylor Childers, Corey Adams, Kyle Felker, Varuni Sastry, Sam Foreman, Archit Vasan, Carlo Graziani, Tanwi Mallick, and Venkat Vishwanath)\n",
    "## Outline \n",
    "### Day 1\n",
    "#### Evolution of computig systems\n",
    "#### Parallel computing\n",
    "#### Introduction to Deep Learning\n",
    "#### Parallel computing in AI\n",
    "\n",
    "### Day 2\n",
    "#### Brief Introduction to LLMs (no codimg here, so will add section later)\n",
    "#### Hands-on LLM training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on LLM Training\n",
    "\n",
    "\n",
    "\n",
    "***Good practice***: Create and activate a conda (or virtual) environment \n",
    "```conda create -n env_mlss_dnn python=3.9```\n",
    "then on jupyter do new ->terminal\n",
    "\n",
    "```\n",
    " conda activate env_mlss_dnn\n",
    " pip install ipykernel \n",
    " python -m ipykernel install --user --name env_mlss_dnn\n",
    "```\n",
    "\n",
    "then go back to your .ipynb file, change kernel to env_mlss_dnn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/karpathy/nanoGPT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mngom/Desktop/DNNMLSS/nanoGPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/env_mlss_dnn/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/mngom/Desktop/DNNMLSS/nanoGPT'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n",
    "\n",
    "# change to /path/to/your/folder\n",
    "%cd nanogpt\n",
    "\n",
    "# confirm\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "!python3 data/shakespeare_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_shakespeare_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-shakespeare-char'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False # override via command line if you like\n",
      "wandb_project = 'shakespeare-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'shakespeare_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256 # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cpu\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 20\n",
      "Overriding: log_interval = 1\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 500\n",
      "Overriding: lr_decay_iters = 2000\n",
      "Overriding: dropout = 0.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 0.80M\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "num decayed parameter tensors: 18, with 802,944 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.1676, val loss 4.1649\n",
      "iter 0: loss 4.1828, time 1359.78ms, mfu -100.00%\n",
      "iter 1: loss 4.1373, time 104.81ms, mfu -100.00%\n",
      "iter 2: loss 4.1347, time 101.26ms, mfu -100.00%\n",
      "iter 3: loss 4.0995, time 99.92ms, mfu -100.00%\n",
      "iter 4: loss 4.0387, time 82.24ms, mfu -100.00%\n",
      "iter 5: loss 3.9758, time 83.77ms, mfu 0.02%\n",
      "iter 6: loss 3.9126, time 107.84ms, mfu 0.01%\n",
      "iter 7: loss 3.8463, time 81.45ms, mfu 0.01%\n",
      "iter 8: loss 3.8169, time 105.95ms, mfu 0.01%\n",
      "iter 9: loss 3.7311, time 102.51ms, mfu 0.01%\n",
      "iter 10: loss 3.7366, time 81.65ms, mfu 0.01%\n",
      "iter 11: loss 3.7234, time 81.57ms, mfu 0.01%\n",
      "iter 12: loss 3.7059, time 112.11ms, mfu 0.01%\n",
      "iter 13: loss 3.6791, time 104.99ms, mfu 0.01%\n",
      "iter 14: loss 3.6453, time 108.79ms, mfu 0.01%\n",
      "iter 15: loss 3.6266, time 81.92ms, mfu 0.01%\n",
      "iter 16: loss 3.6050, time 101.39ms, mfu 0.01%\n",
      "iter 17: loss 3.6008, time 82.16ms, mfu 0.01%\n",
      "iter 18: loss 3.6244, time 79.50ms, mfu 0.01%\n",
      "iter 19: loss 3.6423, time 123.27ms, mfu 0.01%\n",
      "iter 20: loss 3.5233, time 93.18ms, mfu 0.01%\n",
      "iter 21: loss 3.5387, time 104.55ms, mfu 0.01%\n",
      "iter 22: loss 3.4933, time 106.19ms, mfu 0.01%\n",
      "iter 23: loss 3.4671, time 105.17ms, mfu 0.01%\n",
      "iter 24: loss 3.4993, time 103.56ms, mfu 0.01%\n",
      "iter 25: loss 3.3414, time 104.29ms, mfu 0.01%\n",
      "iter 26: loss 3.3209, time 106.14ms, mfu 0.01%\n",
      "iter 27: loss 3.3452, time 107.45ms, mfu 0.01%\n",
      "iter 28: loss 3.2668, time 117.03ms, mfu 0.01%\n",
      "iter 29: loss 3.2942, time 111.63ms, mfu 0.01%\n",
      "iter 30: loss 3.3294, time 82.26ms, mfu 0.01%\n",
      "iter 31: loss 3.2203, time 106.72ms, mfu 0.01%\n",
      "iter 32: loss 3.1959, time 84.90ms, mfu 0.01%\n",
      "iter 33: loss 3.2006, time 103.76ms, mfu 0.01%\n",
      "iter 34: loss 3.1428, time 83.82ms, mfu 0.01%\n",
      "iter 35: loss 3.2393, time 81.74ms, mfu 0.01%\n",
      "iter 36: loss 3.1355, time 101.80ms, mfu 0.01%\n",
      "iter 37: loss 3.1533, time 85.00ms, mfu 0.01%\n",
      "iter 38: loss 3.0117, time 111.31ms, mfu 0.01%\n",
      "iter 39: loss 3.0625, time 80.58ms, mfu 0.01%\n",
      "iter 40: loss 3.0920, time 82.33ms, mfu 0.01%\n",
      "iter 41: loss 3.0309, time 109.72ms, mfu 0.01%\n",
      "iter 42: loss 2.9807, time 80.98ms, mfu 0.01%\n",
      "iter 43: loss 2.9671, time 103.95ms, mfu 0.01%\n",
      "iter 44: loss 3.0005, time 106.43ms, mfu 0.01%\n",
      "iter 45: loss 2.9795, time 111.25ms, mfu 0.01%\n",
      "iter 46: loss 2.9250, time 106.72ms, mfu 0.01%\n",
      "iter 47: loss 3.0510, time 108.86ms, mfu 0.01%\n",
      "iter 48: loss 2.9333, time 80.98ms, mfu 0.01%\n",
      "iter 49: loss 2.9340, time 108.48ms, mfu 0.01%\n",
      "iter 50: loss 2.9336, time 77.97ms, mfu 0.01%\n",
      "iter 51: loss 2.9030, time 108.57ms, mfu 0.01%\n",
      "iter 52: loss 2.9111, time 81.68ms, mfu 0.01%\n",
      "iter 53: loss 2.8949, time 80.11ms, mfu 0.01%\n",
      "iter 54: loss 2.8129, time 107.13ms, mfu 0.01%\n",
      "iter 55: loss 2.8076, time 82.27ms, mfu 0.01%\n",
      "iter 56: loss 2.7797, time 108.22ms, mfu 0.01%\n",
      "iter 57: loss 2.8472, time 105.38ms, mfu 0.01%\n",
      "iter 58: loss 2.8127, time 81.87ms, mfu 0.01%\n",
      "iter 59: loss 2.8339, time 79.19ms, mfu 0.01%\n",
      "iter 60: loss 2.7890, time 105.27ms, mfu 0.01%\n",
      "iter 61: loss 2.8220, time 79.33ms, mfu 0.01%\n",
      "iter 62: loss 2.7516, time 83.42ms, mfu 0.01%\n",
      "iter 63: loss 2.7931, time 84.11ms, mfu 0.01%\n",
      "iter 64: loss 2.7913, time 108.61ms, mfu 0.01%\n",
      "iter 65: loss 2.7310, time 82.89ms, mfu 0.01%\n",
      "iter 66: loss 2.7577, time 113.99ms, mfu 0.01%\n",
      "iter 67: loss 2.7374, time 106.04ms, mfu 0.01%\n",
      "iter 68: loss 2.7673, time 81.28ms, mfu 0.01%\n",
      "iter 69: loss 2.7820, time 103.75ms, mfu 0.01%\n",
      "iter 70: loss 2.7069, time 81.35ms, mfu 0.01%\n",
      "iter 71: loss 2.7687, time 107.58ms, mfu 0.01%\n",
      "iter 72: loss 2.6593, time 82.36ms, mfu 0.01%\n",
      "iter 73: loss 2.7034, time 85.86ms, mfu 0.01%\n",
      "iter 74: loss 2.6512, time 110.53ms, mfu 0.01%\n",
      "iter 75: loss 2.7666, time 82.81ms, mfu 0.01%\n",
      "iter 76: loss 2.6618, time 110.45ms, mfu 0.01%\n",
      "iter 77: loss 2.7576, time 89.86ms, mfu 0.01%\n",
      "iter 78: loss 2.6682, time 93.09ms, mfu 0.01%\n",
      "iter 79: loss 2.6772, time 82.40ms, mfu 0.01%\n",
      "iter 80: loss 2.6128, time 104.48ms, mfu 0.01%\n",
      "iter 81: loss 2.6870, time 84.20ms, mfu 0.01%\n",
      "iter 82: loss 2.6893, time 112.61ms, mfu 0.01%\n",
      "iter 83: loss 2.6264, time 91.22ms, mfu 0.01%\n",
      "iter 84: loss 2.7267, time 98.43ms, mfu 0.01%\n",
      "iter 85: loss 2.6810, time 121.36ms, mfu 0.01%\n",
      "iter 86: loss 2.5991, time 88.07ms, mfu 0.01%\n",
      "iter 87: loss 2.6400, time 84.08ms, mfu 0.01%\n",
      "iter 88: loss 2.6090, time 86.00ms, mfu 0.01%\n",
      "iter 89: loss 2.7311, time 113.94ms, mfu 0.01%\n",
      "iter 90: loss 2.6146, time 85.73ms, mfu 0.01%\n",
      "iter 91: loss 2.6667, time 107.49ms, mfu 0.01%\n",
      "iter 92: loss 2.6025, time 110.73ms, mfu 0.01%\n",
      "iter 93: loss 2.5399, time 106.72ms, mfu 0.01%\n",
      "iter 94: loss 2.7116, time 82.30ms, mfu 0.01%\n",
      "iter 95: loss 2.6400, time 111.12ms, mfu 0.01%\n",
      "iter 96: loss 2.6739, time 89.01ms, mfu 0.01%\n",
      "iter 97: loss 2.5839, time 81.14ms, mfu 0.01%\n",
      "iter 98: loss 2.5320, time 82.75ms, mfu 0.01%\n",
      "iter 99: loss 2.6740, time 108.16ms, mfu 0.01%\n",
      "iter 100: loss 2.6635, time 82.76ms, mfu 0.01%\n",
      "iter 101: loss 2.6532, time 112.92ms, mfu 0.01%\n",
      "iter 102: loss 2.5912, time 85.52ms, mfu 0.01%\n",
      "iter 103: loss 2.6308, time 83.94ms, mfu 0.01%\n",
      "iter 104: loss 2.5048, time 102.89ms, mfu 0.01%\n",
      "iter 105: loss 2.5748, time 82.32ms, mfu 0.01%\n",
      "iter 106: loss 2.5280, time 82.47ms, mfu 0.01%\n",
      "iter 107: loss 2.5885, time 87.99ms, mfu 0.01%\n",
      "iter 108: loss 2.5621, time 106.77ms, mfu 0.01%\n",
      "iter 109: loss 2.5919, time 107.22ms, mfu 0.01%\n",
      "iter 110: loss 2.5880, time 84.76ms, mfu 0.01%\n",
      "iter 111: loss 2.5340, time 114.32ms, mfu 0.01%\n",
      "iter 112: loss 2.5389, time 83.97ms, mfu 0.01%\n",
      "iter 113: loss 2.5485, time 107.63ms, mfu 0.01%\n",
      "iter 114: loss 2.6036, time 108.30ms, mfu 0.01%\n",
      "iter 115: loss 2.5520, time 80.92ms, mfu 0.01%\n",
      "iter 116: loss 2.5809, time 84.93ms, mfu 0.01%\n",
      "iter 117: loss 2.5443, time 112.21ms, mfu 0.01%\n",
      "iter 118: loss 2.5850, time 82.98ms, mfu 0.01%\n",
      "iter 119: loss 2.6244, time 86.58ms, mfu 0.01%\n",
      "iter 120: loss 2.6186, time 103.94ms, mfu 0.01%\n",
      "iter 121: loss 2.7192, time 105.83ms, mfu 0.01%\n",
      "iter 122: loss 2.6091, time 110.38ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 123: loss 2.5891, time 84.77ms, mfu 0.01%\n",
      "iter 124: loss 2.6232, time 112.49ms, mfu 0.01%\n",
      "iter 125: loss 2.5940, time 82.53ms, mfu 0.01%\n",
      "iter 126: loss 2.6158, time 110.00ms, mfu 0.01%\n",
      "iter 127: loss 2.6776, time 84.94ms, mfu 0.01%\n",
      "iter 128: loss 2.5884, time 109.07ms, mfu 0.01%\n",
      "iter 129: loss 2.5599, time 85.11ms, mfu 0.01%\n",
      "iter 130: loss 2.5422, time 110.80ms, mfu 0.01%\n",
      "iter 131: loss 2.6984, time 109.94ms, mfu 0.01%\n",
      "iter 132: loss 2.5189, time 106.72ms, mfu 0.01%\n",
      "iter 133: loss 2.5712, time 114.71ms, mfu 0.01%\n",
      "iter 134: loss 2.4379, time 81.91ms, mfu 0.01%\n",
      "iter 135: loss 2.5714, time 117.05ms, mfu 0.01%\n",
      "iter 136: loss 2.5871, time 80.65ms, mfu 0.01%\n",
      "iter 137: loss 2.5021, time 112.19ms, mfu 0.01%\n",
      "iter 138: loss 2.5119, time 79.72ms, mfu 0.01%\n",
      "iter 139: loss 2.5801, time 104.78ms, mfu 0.01%\n",
      "iter 140: loss 2.4921, time 106.36ms, mfu 0.01%\n",
      "iter 141: loss 2.5164, time 86.11ms, mfu 0.01%\n",
      "iter 142: loss 2.5426, time 118.90ms, mfu 0.01%\n",
      "iter 143: loss 2.5652, time 86.09ms, mfu 0.01%\n",
      "iter 144: loss 2.5373, time 110.80ms, mfu 0.01%\n",
      "iter 145: loss 2.5433, time 83.75ms, mfu 0.01%\n",
      "iter 146: loss 2.5070, time 110.44ms, mfu 0.01%\n",
      "iter 147: loss 2.5597, time 83.32ms, mfu 0.01%\n",
      "iter 148: loss 2.4607, time 84.41ms, mfu 0.01%\n",
      "iter 149: loss 2.5515, time 84.35ms, mfu 0.01%\n",
      "iter 150: loss 2.5425, time 116.00ms, mfu 0.01%\n",
      "iter 151: loss 2.5234, time 82.58ms, mfu 0.01%\n",
      "iter 152: loss 2.5735, time 104.10ms, mfu 0.01%\n",
      "iter 153: loss 2.4897, time 96.61ms, mfu 0.01%\n",
      "iter 154: loss 2.4569, time 112.44ms, mfu 0.01%\n",
      "iter 155: loss 2.4786, time 112.34ms, mfu 0.01%\n",
      "iter 156: loss 2.5061, time 89.38ms, mfu 0.01%\n",
      "iter 157: loss 2.5567, time 86.54ms, mfu 0.01%\n",
      "iter 158: loss 2.5771, time 83.29ms, mfu 0.01%\n",
      "iter 159: loss 2.5045, time 105.97ms, mfu 0.01%\n",
      "iter 160: loss 2.5243, time 84.22ms, mfu 0.01%\n",
      "iter 161: loss 2.4993, time 81.76ms, mfu 0.01%\n",
      "iter 162: loss 2.4630, time 119.60ms, mfu 0.01%\n",
      "iter 163: loss 2.4979, time 108.04ms, mfu 0.01%\n",
      "iter 164: loss 2.5054, time 81.66ms, mfu 0.01%\n",
      "iter 165: loss 2.6246, time 84.41ms, mfu 0.01%\n",
      "iter 166: loss 2.5134, time 82.95ms, mfu 0.01%\n",
      "iter 167: loss 2.4725, time 87.13ms, mfu 0.01%\n",
      "iter 168: loss 2.5520, time 84.10ms, mfu 0.01%\n",
      "iter 169: loss 2.4899, time 113.35ms, mfu 0.01%\n",
      "iter 170: loss 2.4741, time 103.62ms, mfu 0.01%\n",
      "iter 171: loss 2.3834, time 112.87ms, mfu 0.01%\n",
      "iter 172: loss 2.5202, time 80.37ms, mfu 0.01%\n",
      "iter 173: loss 2.5233, time 81.87ms, mfu 0.01%\n",
      "iter 174: loss 2.4690, time 110.48ms, mfu 0.01%\n",
      "iter 175: loss 2.4643, time 115.79ms, mfu 0.01%\n",
      "iter 176: loss 2.4494, time 85.98ms, mfu 0.01%\n",
      "iter 177: loss 2.4908, time 105.02ms, mfu 0.01%\n",
      "iter 178: loss 2.4802, time 83.97ms, mfu 0.01%\n",
      "iter 179: loss 2.6177, time 108.16ms, mfu 0.01%\n",
      "iter 180: loss 2.4928, time 83.35ms, mfu 0.01%\n",
      "iter 181: loss 2.4498, time 109.48ms, mfu 0.01%\n",
      "iter 182: loss 2.4439, time 105.65ms, mfu 0.01%\n",
      "iter 183: loss 2.5471, time 82.43ms, mfu 0.01%\n",
      "iter 184: loss 2.4470, time 83.75ms, mfu 0.01%\n",
      "iter 185: loss 2.5639, time 105.57ms, mfu 0.01%\n",
      "iter 186: loss 2.5656, time 86.54ms, mfu 0.01%\n",
      "iter 187: loss 2.5070, time 82.91ms, mfu 0.01%\n",
      "iter 188: loss 2.4840, time 85.37ms, mfu 0.01%\n",
      "iter 189: loss 2.5641, time 112.76ms, mfu 0.01%\n",
      "iter 190: loss 2.5444, time 83.90ms, mfu 0.01%\n",
      "iter 191: loss 2.5446, time 109.01ms, mfu 0.01%\n",
      "iter 192: loss 2.4553, time 82.56ms, mfu 0.01%\n",
      "iter 193: loss 2.5763, time 86.37ms, mfu 0.01%\n",
      "iter 194: loss 2.4801, time 87.90ms, mfu 0.01%\n",
      "iter 195: loss 2.4763, time 86.56ms, mfu 0.01%\n",
      "iter 196: loss 2.4731, time 107.69ms, mfu 0.01%\n",
      "iter 197: loss 2.4632, time 88.63ms, mfu 0.01%\n",
      "iter 198: loss 2.4527, time 86.09ms, mfu 0.01%\n",
      "iter 199: loss 2.4878, time 86.19ms, mfu 0.01%\n",
      "iter 200: loss 2.5039, time 87.40ms, mfu 0.01%\n",
      "iter 201: loss 2.4361, time 82.87ms, mfu 0.01%\n",
      "iter 202: loss 2.4518, time 118.87ms, mfu 0.01%\n",
      "iter 203: loss 2.5402, time 148.19ms, mfu 0.01%\n",
      "iter 204: loss 2.4754, time 119.76ms, mfu 0.01%\n",
      "iter 205: loss 2.4789, time 108.84ms, mfu 0.01%\n",
      "iter 206: loss 2.4742, time 106.31ms, mfu 0.01%\n",
      "iter 207: loss 2.5718, time 88.31ms, mfu 0.01%\n",
      "iter 208: loss 2.3844, time 85.84ms, mfu 0.01%\n",
      "iter 209: loss 2.5291, time 83.73ms, mfu 0.01%\n",
      "iter 210: loss 2.4330, time 109.68ms, mfu 0.01%\n",
      "iter 211: loss 2.5309, time 83.02ms, mfu 0.01%\n",
      "iter 212: loss 2.4654, time 113.30ms, mfu 0.01%\n",
      "iter 213: loss 2.4521, time 110.92ms, mfu 0.01%\n",
      "iter 214: loss 2.5320, time 105.04ms, mfu 0.01%\n",
      "iter 215: loss 2.5425, time 89.82ms, mfu 0.01%\n",
      "iter 216: loss 2.5085, time 110.33ms, mfu 0.01%\n",
      "iter 217: loss 2.4557, time 106.54ms, mfu 0.01%\n",
      "iter 218: loss 2.5043, time 85.81ms, mfu 0.01%\n",
      "iter 219: loss 2.4581, time 85.30ms, mfu 0.01%\n",
      "iter 220: loss 2.4903, time 110.84ms, mfu 0.01%\n",
      "iter 221: loss 2.5044, time 110.52ms, mfu 0.01%\n",
      "iter 222: loss 2.4566, time 87.21ms, mfu 0.01%\n",
      "iter 223: loss 2.5313, time 107.60ms, mfu 0.01%\n",
      "iter 224: loss 2.3978, time 107.25ms, mfu 0.01%\n",
      "iter 225: loss 2.5483, time 110.59ms, mfu 0.01%\n",
      "iter 226: loss 2.4627, time 87.60ms, mfu 0.01%\n",
      "iter 227: loss 2.4574, time 113.65ms, mfu 0.01%\n",
      "iter 228: loss 2.4476, time 85.12ms, mfu 0.01%\n",
      "iter 229: loss 2.5279, time 83.95ms, mfu 0.01%\n",
      "iter 230: loss 2.4648, time 83.10ms, mfu 0.01%\n",
      "iter 231: loss 2.4212, time 83.89ms, mfu 0.01%\n",
      "iter 232: loss 2.4458, time 81.78ms, mfu 0.01%\n",
      "iter 233: loss 2.4612, time 83.78ms, mfu 0.01%\n",
      "iter 234: loss 2.4193, time 108.21ms, mfu 0.01%\n",
      "iter 235: loss 2.4913, time 110.52ms, mfu 0.01%\n",
      "iter 236: loss 2.4356, time 106.49ms, mfu 0.01%\n",
      "iter 237: loss 2.4827, time 106.47ms, mfu 0.01%\n",
      "iter 238: loss 2.5102, time 107.84ms, mfu 0.01%\n",
      "iter 239: loss 2.4670, time 110.44ms, mfu 0.01%\n",
      "iter 240: loss 2.5144, time 84.00ms, mfu 0.01%\n",
      "iter 241: loss 2.4216, time 110.69ms, mfu 0.01%\n",
      "iter 242: loss 2.4851, time 86.21ms, mfu 0.01%\n",
      "iter 243: loss 2.4914, time 86.90ms, mfu 0.01%\n",
      "iter 244: loss 2.4181, time 109.19ms, mfu 0.01%\n",
      "iter 245: loss 2.5124, time 84.21ms, mfu 0.01%\n",
      "iter 246: loss 2.4154, time 83.70ms, mfu 0.01%\n",
      "iter 247: loss 2.4658, time 108.09ms, mfu 0.01%\n",
      "iter 248: loss 2.4475, time 86.63ms, mfu 0.01%\n",
      "iter 249: loss 2.4449, time 108.43ms, mfu 0.01%\n",
      "step 250: train loss 2.4293, val loss 2.4447\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 250: loss 2.4904, time 1468.65ms, mfu 0.01%\n",
      "iter 251: loss 2.3638, time 118.02ms, mfu 0.01%\n",
      "iter 252: loss 2.5004, time 82.07ms, mfu 0.01%\n",
      "iter 253: loss 2.4087, time 81.69ms, mfu 0.01%\n",
      "iter 254: loss 2.3968, time 114.83ms, mfu 0.01%\n",
      "iter 255: loss 2.4558, time 84.30ms, mfu 0.01%\n",
      "iter 256: loss 2.3588, time 83.39ms, mfu 0.01%\n",
      "iter 257: loss 2.3921, time 108.43ms, mfu 0.01%\n",
      "iter 258: loss 2.4123, time 85.37ms, mfu 0.01%\n",
      "iter 259: loss 2.3825, time 84.96ms, mfu 0.01%\n",
      "iter 260: loss 2.4461, time 82.97ms, mfu 0.01%\n",
      "iter 261: loss 2.4801, time 107.00ms, mfu 0.01%\n",
      "iter 262: loss 2.4433, time 116.01ms, mfu 0.01%\n",
      "iter 263: loss 2.4063, time 85.12ms, mfu 0.01%\n",
      "iter 264: loss 2.4662, time 106.59ms, mfu 0.01%\n",
      "iter 265: loss 2.4957, time 86.06ms, mfu 0.01%\n",
      "iter 266: loss 2.4313, time 84.33ms, mfu 0.01%\n",
      "iter 267: loss 2.3545, time 111.96ms, mfu 0.01%\n",
      "iter 268: loss 2.5298, time 82.04ms, mfu 0.01%\n",
      "iter 269: loss 2.4722, time 111.69ms, mfu 0.01%\n",
      "iter 270: loss 2.4044, time 109.48ms, mfu 0.01%\n",
      "iter 271: loss 2.4732, time 83.22ms, mfu 0.01%\n",
      "iter 272: loss 2.4242, time 84.85ms, mfu 0.01%\n",
      "iter 273: loss 2.3979, time 109.82ms, mfu 0.01%\n",
      "iter 274: loss 2.4059, time 110.66ms, mfu 0.01%\n",
      "iter 275: loss 2.4296, time 83.63ms, mfu 0.01%\n",
      "iter 276: loss 2.4052, time 112.09ms, mfu 0.01%\n",
      "iter 277: loss 2.3876, time 105.58ms, mfu 0.01%\n",
      "iter 278: loss 2.3873, time 104.88ms, mfu 0.01%\n",
      "iter 279: loss 2.3665, time 109.63ms, mfu 0.01%\n",
      "iter 280: loss 2.3788, time 130.48ms, mfu 0.01%\n",
      "iter 281: loss 2.4736, time 112.55ms, mfu 0.01%\n",
      "iter 282: loss 2.4060, time 109.58ms, mfu 0.01%\n",
      "iter 283: loss 2.4246, time 112.37ms, mfu 0.01%\n",
      "iter 284: loss 2.3952, time 106.08ms, mfu 0.01%\n",
      "iter 285: loss 2.4339, time 90.20ms, mfu 0.01%\n",
      "iter 286: loss 2.4566, time 87.06ms, mfu 0.01%\n",
      "iter 287: loss 2.4284, time 108.47ms, mfu 0.01%\n",
      "iter 288: loss 2.4340, time 87.70ms, mfu 0.01%\n",
      "iter 289: loss 2.3849, time 106.62ms, mfu 0.01%\n",
      "iter 290: loss 2.3959, time 110.38ms, mfu 0.01%\n",
      "iter 291: loss 2.3171, time 86.69ms, mfu 0.01%\n",
      "iter 292: loss 2.3974, time 103.40ms, mfu 0.01%\n",
      "iter 293: loss 2.3744, time 115.99ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 294: loss 2.3162, time 145.47ms, mfu 0.01%\n",
      "iter 295: loss 2.4403, time 121.91ms, mfu 0.01%\n",
      "iter 296: loss 2.3260, time 88.06ms, mfu 0.01%\n",
      "iter 297: loss 2.3889, time 86.20ms, mfu 0.01%\n",
      "iter 298: loss 2.3428, time 113.88ms, mfu 0.01%\n",
      "iter 299: loss 2.3893, time 85.86ms, mfu 0.01%\n",
      "iter 300: loss 2.3350, time 106.29ms, mfu 0.01%\n",
      "iter 301: loss 2.2811, time 109.80ms, mfu 0.01%\n",
      "iter 302: loss 2.4488, time 112.75ms, mfu 0.01%\n",
      "iter 303: loss 2.4061, time 115.12ms, mfu 0.01%\n",
      "iter 304: loss 2.4852, time 85.64ms, mfu 0.01%\n",
      "iter 305: loss 2.4113, time 113.03ms, mfu 0.01%\n",
      "iter 306: loss 2.3722, time 86.60ms, mfu 0.01%\n",
      "iter 307: loss 2.3499, time 87.37ms, mfu 0.01%\n",
      "iter 308: loss 2.4330, time 87.07ms, mfu 0.01%\n",
      "iter 309: loss 2.4252, time 88.45ms, mfu 0.01%\n",
      "iter 310: loss 2.3489, time 117.69ms, mfu 0.01%\n",
      "iter 311: loss 2.4107, time 107.91ms, mfu 0.01%\n",
      "iter 312: loss 2.4517, time 128.59ms, mfu 0.01%\n",
      "iter 313: loss 2.4372, time 94.48ms, mfu 0.01%\n",
      "iter 314: loss 2.4837, time 88.61ms, mfu 0.01%\n",
      "iter 315: loss 2.4368, time 99.60ms, mfu 0.01%\n",
      "iter 316: loss 2.4451, time 107.03ms, mfu 0.01%\n",
      "iter 317: loss 2.2934, time 90.38ms, mfu 0.01%\n",
      "iter 318: loss 2.3787, time 84.16ms, mfu 0.01%\n",
      "iter 319: loss 2.4132, time 110.70ms, mfu 0.01%\n",
      "iter 320: loss 2.4062, time 107.42ms, mfu 0.01%\n",
      "iter 321: loss 2.3898, time 161.98ms, mfu 0.01%\n",
      "iter 322: loss 2.3933, time 105.11ms, mfu 0.01%\n",
      "iter 323: loss 2.4027, time 111.53ms, mfu 0.01%\n",
      "iter 324: loss 2.3942, time 86.55ms, mfu 0.01%\n",
      "iter 325: loss 2.3657, time 116.96ms, mfu 0.01%\n",
      "iter 326: loss 2.4684, time 107.66ms, mfu 0.01%\n",
      "iter 327: loss 2.3654, time 89.73ms, mfu 0.01%\n",
      "iter 328: loss 2.3823, time 106.05ms, mfu 0.01%\n",
      "iter 329: loss 2.3280, time 157.57ms, mfu 0.01%\n",
      "iter 330: loss 2.3390, time 132.57ms, mfu 0.01%\n",
      "iter 331: loss 2.4196, time 87.59ms, mfu 0.01%\n",
      "iter 332: loss 2.3537, time 91.70ms, mfu 0.01%\n",
      "iter 333: loss 2.4219, time 89.35ms, mfu 0.01%\n",
      "iter 334: loss 2.3949, time 118.20ms, mfu 0.01%\n",
      "iter 335: loss 2.3995, time 91.94ms, mfu 0.01%\n",
      "iter 336: loss 2.4226, time 107.42ms, mfu 0.01%\n",
      "iter 337: loss 2.3236, time 98.93ms, mfu 0.01%\n",
      "iter 338: loss 2.3643, time 114.21ms, mfu 0.01%\n",
      "iter 339: loss 2.4672, time 87.62ms, mfu 0.01%\n",
      "iter 340: loss 2.3313, time 85.77ms, mfu 0.01%\n",
      "iter 341: loss 2.3150, time 144.32ms, mfu 0.01%\n",
      "iter 342: loss 2.3031, time 89.78ms, mfu 0.01%\n",
      "iter 343: loss 2.3194, time 88.73ms, mfu 0.01%\n",
      "iter 344: loss 2.4064, time 105.76ms, mfu 0.01%\n",
      "iter 345: loss 2.3602, time 115.71ms, mfu 0.01%\n",
      "iter 346: loss 2.3595, time 81.68ms, mfu 0.01%\n",
      "iter 347: loss 2.3857, time 93.68ms, mfu 0.01%\n",
      "iter 348: loss 2.3178, time 85.34ms, mfu 0.01%\n",
      "iter 349: loss 2.3682, time 98.73ms, mfu 0.01%\n",
      "iter 350: loss 2.3375, time 90.17ms, mfu 0.01%\n",
      "iter 351: loss 2.2778, time 139.66ms, mfu 0.01%\n",
      "iter 352: loss 2.3887, time 84.37ms, mfu 0.01%\n",
      "iter 353: loss 2.4588, time 88.66ms, mfu 0.01%\n",
      "iter 354: loss 2.3892, time 116.11ms, mfu 0.01%\n",
      "iter 355: loss 2.4674, time 118.41ms, mfu 0.01%\n",
      "iter 356: loss 2.2908, time 85.24ms, mfu 0.01%\n",
      "iter 357: loss 2.3011, time 108.85ms, mfu 0.01%\n",
      "iter 358: loss 2.3417, time 105.41ms, mfu 0.01%\n",
      "iter 359: loss 2.3650, time 114.98ms, mfu 0.01%\n",
      "iter 360: loss 2.4137, time 101.21ms, mfu 0.01%\n",
      "iter 361: loss 2.2958, time 154.84ms, mfu 0.01%\n",
      "iter 362: loss 2.3831, time 83.89ms, mfu 0.01%\n",
      "iter 363: loss 2.3644, time 105.01ms, mfu 0.01%\n",
      "iter 364: loss 2.3618, time 108.31ms, mfu 0.01%\n",
      "iter 365: loss 2.3879, time 115.62ms, mfu 0.01%\n",
      "iter 366: loss 2.4220, time 87.15ms, mfu 0.01%\n",
      "iter 367: loss 2.3350, time 116.74ms, mfu 0.01%\n",
      "iter 368: loss 2.3737, time 105.65ms, mfu 0.01%\n",
      "iter 369: loss 2.3915, time 121.19ms, mfu 0.01%\n",
      "iter 370: loss 2.4959, time 156.59ms, mfu 0.01%\n",
      "iter 371: loss 2.3696, time 87.84ms, mfu 0.01%\n",
      "iter 372: loss 2.3158, time 107.17ms, mfu 0.01%\n",
      "iter 373: loss 2.3926, time 116.07ms, mfu 0.01%\n",
      "iter 374: loss 2.3979, time 109.97ms, mfu 0.01%\n",
      "iter 375: loss 2.2866, time 106.47ms, mfu 0.01%\n",
      "iter 376: loss 2.3689, time 93.16ms, mfu 0.01%\n",
      "iter 377: loss 2.3612, time 103.45ms, mfu 0.01%\n",
      "iter 378: loss 2.3669, time 110.19ms, mfu 0.01%\n",
      "iter 379: loss 2.3135, time 104.57ms, mfu 0.01%\n",
      "iter 380: loss 2.3445, time 138.06ms, mfu 0.01%\n",
      "iter 381: loss 2.2895, time 106.83ms, mfu 0.01%\n",
      "iter 382: loss 2.4056, time 106.88ms, mfu 0.01%\n",
      "iter 383: loss 2.3747, time 104.44ms, mfu 0.01%\n",
      "iter 384: loss 2.2996, time 84.74ms, mfu 0.01%\n",
      "iter 385: loss 2.3016, time 111.47ms, mfu 0.01%\n",
      "iter 386: loss 2.3440, time 97.44ms, mfu 0.01%\n",
      "iter 387: loss 2.3218, time 88.34ms, mfu 0.01%\n",
      "iter 388: loss 2.2424, time 121.46ms, mfu 0.01%\n",
      "iter 389: loss 2.3601, time 105.91ms, mfu 0.01%\n",
      "iter 390: loss 2.3626, time 161.21ms, mfu 0.01%\n",
      "iter 391: loss 2.4102, time 85.63ms, mfu 0.01%\n",
      "iter 392: loss 2.2965, time 105.41ms, mfu 0.01%\n",
      "iter 393: loss 2.3174, time 112.44ms, mfu 0.01%\n",
      "iter 394: loss 2.4587, time 111.56ms, mfu 0.01%\n",
      "iter 395: loss 2.3097, time 108.76ms, mfu 0.01%\n",
      "iter 396: loss 2.3778, time 106.42ms, mfu 0.01%\n",
      "iter 397: loss 2.3060, time 109.55ms, mfu 0.01%\n",
      "iter 398: loss 2.2219, time 112.81ms, mfu 0.01%\n",
      "iter 399: loss 2.3750, time 104.26ms, mfu 0.01%\n",
      "iter 400: loss 2.3405, time 156.67ms, mfu 0.01%\n",
      "iter 401: loss 2.3534, time 107.91ms, mfu 0.01%\n",
      "iter 402: loss 2.3740, time 108.82ms, mfu 0.01%\n",
      "iter 403: loss 2.3630, time 91.98ms, mfu 0.01%\n",
      "iter 404: loss 2.3433, time 118.92ms, mfu 0.01%\n",
      "iter 405: loss 2.3948, time 105.92ms, mfu 0.01%\n",
      "iter 406: loss 2.3297, time 166.54ms, mfu 0.01%\n",
      "iter 407: loss 2.3815, time 119.36ms, mfu 0.01%\n",
      "iter 408: loss 2.2795, time 111.91ms, mfu 0.01%\n",
      "iter 409: loss 2.3073, time 112.16ms, mfu 0.01%\n",
      "iter 410: loss 2.2802, time 112.12ms, mfu 0.01%\n",
      "iter 411: loss 2.3615, time 111.67ms, mfu 0.01%\n",
      "iter 412: loss 2.3462, time 85.37ms, mfu 0.01%\n",
      "iter 413: loss 2.3275, time 119.01ms, mfu 0.01%\n",
      "iter 414: loss 2.3518, time 105.59ms, mfu 0.01%\n",
      "iter 415: loss 2.4136, time 108.77ms, mfu 0.01%\n",
      "iter 416: loss 2.3288, time 109.04ms, mfu 0.01%\n",
      "iter 417: loss 2.3052, time 109.90ms, mfu 0.01%\n",
      "iter 418: loss 2.2778, time 108.95ms, mfu 0.01%\n",
      "iter 419: loss 2.2965, time 82.16ms, mfu 0.01%\n",
      "iter 420: loss 2.3759, time 84.91ms, mfu 0.01%\n",
      "iter 421: loss 2.2363, time 114.60ms, mfu 0.01%\n",
      "iter 422: loss 2.2847, time 118.32ms, mfu 0.01%\n",
      "iter 423: loss 2.3250, time 105.02ms, mfu 0.01%\n",
      "iter 424: loss 2.2748, time 82.95ms, mfu 0.01%\n",
      "iter 425: loss 2.3195, time 106.24ms, mfu 0.01%\n",
      "iter 426: loss 2.4147, time 108.28ms, mfu 0.01%\n",
      "iter 427: loss 2.3659, time 111.05ms, mfu 0.01%\n",
      "iter 428: loss 2.3075, time 111.34ms, mfu 0.01%\n",
      "iter 429: loss 2.2580, time 114.83ms, mfu 0.01%\n",
      "iter 430: loss 2.4103, time 113.39ms, mfu 0.01%\n",
      "iter 431: loss 2.3262, time 116.81ms, mfu 0.01%\n",
      "iter 432: loss 2.3371, time 117.14ms, mfu 0.01%\n",
      "iter 433: loss 2.2726, time 112.66ms, mfu 0.01%\n",
      "iter 434: loss 2.2847, time 107.53ms, mfu 0.01%\n",
      "iter 435: loss 2.2916, time 114.41ms, mfu 0.01%\n",
      "iter 436: loss 2.3977, time 116.11ms, mfu 0.01%\n",
      "iter 437: loss 2.2551, time 110.01ms, mfu 0.01%\n",
      "iter 438: loss 2.2854, time 115.04ms, mfu 0.01%\n",
      "iter 439: loss 2.2792, time 121.59ms, mfu 0.01%\n",
      "iter 440: loss 2.3480, time 118.70ms, mfu 0.01%\n",
      "iter 441: loss 2.3338, time 115.77ms, mfu 0.01%\n",
      "iter 442: loss 2.3563, time 114.54ms, mfu 0.01%\n",
      "iter 443: loss 2.3662, time 111.62ms, mfu 0.01%\n",
      "iter 444: loss 2.3575, time 115.21ms, mfu 0.01%\n",
      "iter 445: loss 2.2895, time 111.58ms, mfu 0.01%\n",
      "iter 446: loss 2.2954, time 109.59ms, mfu 0.01%\n",
      "iter 447: loss 2.3782, time 114.32ms, mfu 0.01%\n",
      "iter 448: loss 2.3840, time 116.36ms, mfu 0.01%\n",
      "iter 449: loss 2.3348, time 105.29ms, mfu 0.01%\n",
      "iter 450: loss 2.3362, time 112.31ms, mfu 0.01%\n",
      "iter 451: loss 2.2999, time 113.88ms, mfu 0.01%\n",
      "iter 452: loss 2.3507, time 111.83ms, mfu 0.01%\n",
      "iter 453: loss 2.3094, time 114.85ms, mfu 0.01%\n",
      "iter 454: loss 2.3534, time 114.93ms, mfu 0.01%\n",
      "iter 455: loss 2.3407, time 84.56ms, mfu 0.01%\n",
      "iter 456: loss 2.3085, time 86.80ms, mfu 0.01%\n",
      "iter 457: loss 2.2953, time 84.85ms, mfu 0.01%\n",
      "iter 458: loss 2.2863, time 84.95ms, mfu 0.01%\n",
      "iter 459: loss 2.3017, time 84.91ms, mfu 0.01%\n",
      "iter 460: loss 2.3215, time 85.66ms, mfu 0.01%\n",
      "iter 461: loss 2.3446, time 116.82ms, mfu 0.01%\n",
      "iter 462: loss 2.3883, time 114.03ms, mfu 0.01%\n",
      "iter 463: loss 2.3071, time 82.65ms, mfu 0.01%\n",
      "iter 464: loss 2.2828, time 85.18ms, mfu 0.01%\n",
      "iter 465: loss 2.3161, time 83.14ms, mfu 0.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 466: loss 2.2996, time 113.64ms, mfu 0.01%\n",
      "iter 467: loss 2.2252, time 83.15ms, mfu 0.01%\n",
      "iter 468: loss 2.2891, time 84.03ms, mfu 0.01%\n",
      "iter 469: loss 2.2152, time 103.73ms, mfu 0.01%\n",
      "iter 470: loss 2.2820, time 113.13ms, mfu 0.01%\n",
      "iter 471: loss 2.2975, time 104.58ms, mfu 0.01%\n",
      "iter 472: loss 2.2505, time 106.85ms, mfu 0.01%\n",
      "iter 473: loss 2.2848, time 104.97ms, mfu 0.01%\n",
      "iter 474: loss 2.2092, time 86.17ms, mfu 0.01%\n",
      "iter 475: loss 2.3322, time 107.76ms, mfu 0.01%\n",
      "iter 476: loss 2.2793, time 84.62ms, mfu 0.01%\n",
      "iter 477: loss 2.3126, time 117.23ms, mfu 0.01%\n",
      "iter 478: loss 2.3488, time 117.03ms, mfu 0.01%\n",
      "iter 479: loss 2.3008, time 113.05ms, mfu 0.01%\n",
      "iter 480: loss 2.2210, time 118.65ms, mfu 0.01%\n",
      "iter 481: loss 2.1983, time 83.60ms, mfu 0.01%\n",
      "iter 482: loss 2.3162, time 114.07ms, mfu 0.01%\n",
      "iter 483: loss 2.2768, time 84.06ms, mfu 0.01%\n",
      "iter 484: loss 2.2773, time 110.52ms, mfu 0.01%\n",
      "iter 485: loss 2.2800, time 116.42ms, mfu 0.01%\n",
      "iter 486: loss 2.2805, time 106.64ms, mfu 0.01%\n",
      "iter 487: loss 2.3032, time 85.32ms, mfu 0.01%\n",
      "iter 488: loss 2.3084, time 84.26ms, mfu 0.01%\n",
      "iter 489: loss 2.2776, time 83.58ms, mfu 0.01%\n",
      "iter 490: loss 2.3192, time 105.54ms, mfu 0.01%\n",
      "iter 491: loss 2.2949, time 86.87ms, mfu 0.01%\n",
      "iter 492: loss 2.3934, time 109.53ms, mfu 0.01%\n",
      "iter 493: loss 2.1888, time 91.60ms, mfu 0.01%\n",
      "iter 494: loss 2.3493, time 109.86ms, mfu 0.01%\n",
      "iter 495: loss 2.2627, time 84.53ms, mfu 0.01%\n",
      "iter 496: loss 2.2630, time 86.07ms, mfu 0.01%\n",
      "iter 497: loss 2.2483, time 84.81ms, mfu 0.01%\n",
      "iter 498: loss 2.3021, time 110.26ms, mfu 0.01%\n",
      "iter 499: loss 2.2272, time 83.63ms, mfu 0.01%\n",
      "step 500: train loss 2.2732, val loss 2.3141\n",
      "saving checkpoint to out-shakespeare-char\n",
      "iter 500: loss 2.3827, time 1451.85ms, mfu 0.01%\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=500 --lr_decay_iters=2000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-shakespeare-char\n",
      "Overriding: device = cpu\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 0.80M\n",
      "Loading meta from data/shakespeare_char/meta.pkl...\n",
      "\n",
      "WONIO:\n",
      "We what ler tou fams florave aray,\n",
      "Sou atlle au sexproors bly, beio f bunolour shan aure\n",
      "Whin af torter my hasins bows fors min ghay,\n",
      "Ongare aind sours and murons cor tor, af mor ithes op.\n",
      "\n",
      "\n",
      "ELORCEOP:\n",
      "Pe ard, it tofon, te, sou wiapene.\n",
      "\n",
      "LORIUKERI:\n",
      "Th lis lo ir, pis, bof shenm porep, tha sprit makerk'lle.\n",
      "\n",
      "\n",
      "DULIO:\n",
      "Wite lis of ived.\n",
      "My Thally, sthis youll bas beamever intad, the trad save dyoince\n",
      "Se Ostha berion, ar tiste s teand by relowe, tof theatI d Har's bofurerinis.\n",
      "\n",
      "\n",
      "You you VORD\n",
      "SIQ\n",
      "---------------\n",
      "\n",
      "Toprurd, I son boon itrer\n",
      "He beand s he.\n",
      "\n",
      "If lal Sthe har anden Hyour wit ory wally pile.\n",
      "\n",
      "MYow; MORIO:\n",
      "Whes w wins for yo wend,\n",
      "A baneang, thof soour wn, thas.\n",
      "\n",
      "GARY GEDURIO:\n",
      "\n",
      "I I'I sth'she bee la or, too dan?\n",
      "\n",
      "\n",
      "Hay RIOLANTE:\n",
      "We tyou spror tous o, tha bee gangn het!\n",
      "I bilen me, bun apghe hy? yoonch I hais I wison my cheingear\n",
      "ses of lat ars or mand, imy ceaid now, nould by hergean,\n",
      "Tous alllen's dham and, me tha yoult othys my brind\n",
      "Mang tit.\n",
      "Welone Gfoul mar do mever thak:\n",
      "I's but here font ch\n",
      "---------------\n",
      "\n",
      "Towll fou be thear, thal greard\n",
      "Merled an lan thak win! int the int bean\n",
      "The tat us dathes my andt son and ocais con\n",
      "And oilin son\n",
      "Furd ree hou wimounes atyous s thealls,\n",
      "An ir thich you sthel tore fu tound beakht cite;\n",
      "O shen la ser fon trerd veie the spiy oren. \n",
      "\n",
      "Werat ha n ther Year'd dour pelilanat se on tha gefay.\n",
      "\n",
      "\n",
      "FOULMIUSTETA:\n",
      "Te mer puitol the thee insn bos othite nes' sham theane d.\n",
      "Wherdy Clo hin gretand four my,\n",
      "Alend! by lee anfI tof tof angore dithean, sher hest yor porcine.\n",
      "\n",
      "NOLEN\n",
      "---------------\n",
      "\n",
      "LOUS:\n",
      "Tre hat grassing faly.\n",
      "\n",
      "\n",
      "Luronthat LIRCHA:\n",
      "\n",
      "\n",
      "That, is an se the reis is.\n",
      "\n",
      "Fouren!\n",
      "\n",
      "\n",
      "MICARDADAUS:\n",
      "An lil an hou thegler sthe or dee tis;\n",
      "she whe you sthe ard thend erghe tig.\n",
      "\n",
      "\n",
      "Efrour INGLDIO:\n",
      "\n",
      "Wid thas de let roow buthinker, rern tis thaf?\n",
      "\n",
      "\n",
      "NGLGEARDIIUUM:\n",
      "Tat faitlly se, in, lecer the er sheise, yof ain.\n",
      "\n",
      ":\n",
      "Wend heral mut the ist an foll trean, sepe hanen fo my, gen freave being\n",
      "Wham, thare, tof sen it folor gous yof tha forerl coan berd\n",
      "I by's latupht sthe pais moues en fond thes tis loc\n",
      "---------------\n",
      "\n",
      "Tof beany rarn thas you hape,\n",
      "Asind wis 's nof of for ines to louthe an this\n",
      "Ang thel nom.\n",
      "\n",
      "\n",
      "ANE:\n",
      "Hyo raperes ale then be habloued\n",
      "O:\n",
      "The, cour's an the tho she awica douw, buthein\n",
      "Wh In thot s ilou the, foy mo tou a thit hive themo steat\n",
      "Aus dean but you hast, wis bo the the my andis pencen'\n",
      " spamedoy; dun ans quow, ke that af gAnoulys is my lat yore,\n",
      "Whe thalu ime thu aticht I? hear.\n",
      "\n",
      "\n",
      "IOLUCENI:\n",
      "Is, the tou top in than my, he ha thee thaspou s?\n",
      "\n",
      "That tay ORTI wame.\n",
      "LUM:\n",
      "\n",
      "Site oulld aks, this b\n",
      "---------------\n",
      "\n",
      "Maver flaty, tou the thave anghes hand's\n",
      "Sigul meng heus or tots, ther oy a mor yof cand,\n",
      "Tou ther than-se thane he, lan hetit s henun ha ame or hesthe\n",
      "Wharow be hecen the melut coitike s otntat, po touly I my eecee\n",
      "An theras were: burut sow yoour lou's burt nonay ou,\n",
      "I'd around now's lip my thou, then if butheler,\n",
      "Was thit licet s birenene pour thens, lof its broesths greve:\n",
      "A my, ssho he and ther willl la's her hof nof wedy\n",
      "Whall dou sis thang stoous igly tateen, theis sne ar a tiour\n",
      "En, theve\n",
      "---------------\n",
      "\n",
      "Se wis; tou Emard ramed, astanes sil,\n",
      "Wit be toul, Ore ald the trepuntr,\n",
      "I im but pannarou he tovees and thit thes mor deand imas\n",
      "As a oughe o 3 thinghe bun neo stue ind sharen mew\n",
      "Lin ooug so by avis in con is creean, trend herfe epishing:\n",
      "I lon the ear se her and shen\n",
      "And oNo thabulled the tice supstest, ceiewe\n",
      "A sther op an buefow the ind ange uea ors byous wins,\n",
      "This wer thep the iche beaner ind stoff ondy tor ay cour\n",
      "The tow as loer his wher acof arand\n",
      "If ank bevelelt mee ong rener fou, yo \n",
      "---------------\n",
      "\n",
      "Lordy mourght therswe murgh hewisurind ar tguly thacest\n",
      "An meally y fed, the mad freiser hit hear hirer's hank\n",
      "Wy tho you ou your sexppousu le ongs not butarg.\n",
      "\n",
      "\n",
      "BENILE:\n",
      "I how the k, urol sith ler,\n",
      "And heain stu merte heris meve averns cewtas\n",
      "Then hay hy hexs ron byorke mars tod ime shang\n",
      "Inds hexse net the by tou, dat you hen buter wot, berd matellly;\n",
      "The tou, by bee hap thee ais maves hiro sonque bevert;\n",
      "The en inchin kis fecufot, then tho cent my chaameak,\n",
      "Anot hes prorme tof it\n",
      "Liintthe, u c\n",
      "---------------\n",
      "\n",
      "METE: the o lit theear;\n",
      "RI net ha wild'd mahin, hpatalsh ak my hing dis's batt be thallom\n",
      "My s torist nove me bue hin by Inour as shece thime,\n",
      "Whand tof, no he p tlat want beno de selof thin\n",
      "\n",
      "I gris nour hemen the cis bevelleriu amf eaf tolld bo thethe?\n",
      "\n",
      "\n",
      "\n",
      "OZOLGAUSO:\n",
      "Hy bur mele the my bepene ing mad of trelew.\n",
      "\n",
      "\n",
      "\n",
      "KE:\n",
      "Riby ouclek's,\n",
      "Tr thathelin deak lous, ing?\n",
      "\n",
      "\n",
      "I ardut mant ouls thef, me tand, my inc thes by afan\n",
      "Buthers mameseches's rean!\n",
      "Who stou gis ooy, bechersl he foureenss.\n",
      "\n",
      "\n",
      "BEe mathe.\n",
      "\n",
      "---------------\n",
      "\n",
      "Ror so aerns thin las ar.\n",
      "\n",
      "\n",
      "KESES,\n",
      "NTISTHARD:\n",
      "F, proor ay fris fand, halld they, as erfand\n",
      "Aat hore ime mery famat y noo theert se, tat I srecraurisone.\n",
      "\n",
      "\n",
      "EROULO:\n",
      "\n",
      "Tou sis oou, hencle tooou hat the tom:\n",
      "I peo to an burit, mept mous pune for ve.\n",
      "KI Menile sporse he Sprea alre hean len.\n",
      "\n",
      "\n",
      "HIRY at gas heand my:\n",
      "Hons rere itin my rey ou bulld geniAtpe comme? noulloe,\n",
      "By noort one too theres the tore the b dilors mout:\n",
      "We seat theand tou dere pot tyot quare wanoury\n",
      "Ou tane the k nofo palerese seat ti\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-shakespeare-char --device=cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export NCCL_DEBUG=INFO\n",
    "!export NCCL_DEBUG_SUBSYS=ALL\n",
    "!export NCCL_DEBUG_FILE=nccl_trace.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on one NVIDIA T4 Tensor Cores, 4GPUS/node\n",
    "# Bamba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 77, in <module>\n",
      "    exec(open('configurator.py').read()) # overrides from command line or config file\n",
      "  File \"<string>\", line 23, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 77, in <module>\n",
      "AssertionError\n",
      "    exec(open('configurator.py').read()) # overrides from command line or config file\n",
      "  File \"<string>\", line 23, in <module>\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 77, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 77, in <module>\n",
      "    exec(open('configurator.py').read()) # overrides from command line or config file\n",
      "  File \"<string>\", line 23, in <module>\n",
      "AssertionError\n",
      "    exec(open('configurator.py').read()) # overrides from command line or config file\n",
      "  File \"<string>\", line 23, in <module>\n",
      "AssertionError\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 58949) of binary: /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/distributed/run.py\", line 719, in main\n",
      "    run(args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/distributed/run.py\", line 713, in run\n",
      "    )(*cmd_args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/distributed/launcher/api.py\", line 261, in launch_agent\n",
      "    failures=result.failures,\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2025-06-20_11:38:38\n",
      "  host      : macbook-pro-10.lan\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 58950)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[2]:\n",
      "  time      : 2025-06-20_11:38:38\n",
      "  host      : macbook-pro-10.lan\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 58951)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[3]:\n",
      "  time      : 2025-06-20_11:38:38\n",
      "  host      : macbook-pro-10.lan\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 58952)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-06-20_11:38:38\n",
      "  host      : macbook-pro-10.lan\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 58949)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "\n",
    "# launch 4 processes  4 GPUs\n",
    "!torchrun \\\n",
    "  --nproc_per_node=4 \\\n",
    "  train.py \\\n",
    "    --config config/train_shakespeare_char.py \\\n",
    "    --batch_size 12 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --compile=False 2>&1 | tee full_train.log\n",
    "\n",
    "#I am on my personal laptop hence the following error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on N NVIDIA T4 Tensor Cores (4N GPUs), I am not sure how you'd do this on sagemaker. But usually, on each node k, I'd do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
      "/bin/bash: -c: line 0: `export MASTER_ADDR=<IP-of-node-0>'\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "usage: torchrun [-h] [--nnodes NNODES] [--nproc_per_node NPROC_PER_NODE]\n",
      "                [--rdzv_backend RDZV_BACKEND] [--rdzv_endpoint RDZV_ENDPOINT]\n",
      "                [--rdzv_id RDZV_ID] [--rdzv_conf RDZV_CONF] [--standalone]\n",
      "                [--max_restarts MAX_RESTARTS]\n",
      "                [--monitor_interval MONITOR_INTERVAL]\n",
      "                [--start_method {spawn,fork,forkserver}] [--role ROLE] [-m]\n",
      "                [--no_python] [--run_path] [--log_dir LOG_DIR] [-r REDIRECTS]\n",
      "                [-t TEE] [--node_rank NODE_RANK] [--master_addr MASTER_ADDR]\n",
      "                [--master_port MASTER_PORT]\n",
      "                training_script ...\n",
      "torchrun: error: argument --node_rank: invalid int value: 'k'\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1,2,3          \n",
    "!export MASTER_ADDR=<IP-of-node-0>            \n",
    "!export MASTER_PORT=29500                     \n",
    "\n",
    "!torchrun \\\n",
    "  --nnodes=N \\\n",
    "  --node_rank=k \\\n",
    "  --nproc_per_node=GPUS_PER_NODE \\\n",
    "  --master_addr=$MASTER_ADDR \\\n",
    "  --master_port=$MASTER_PORT \\\n",
    "  train.py \\\n",
    "    --config config/train_shakespeare_char.py \\\n",
    "    --batch_size 12 \\\n",
    "    --gradient_accumulation_steps 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZV5UwakJTbqgB0ItPwe9k",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_mlss_dnn",
   "language": "python",
   "name": "env_mlss_dnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
