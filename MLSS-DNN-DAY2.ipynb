{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/IntroLLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel and distributed Deep Learning\n",
    "\n",
    "## Author: Marieme Ngom, Argonne National Laboratory\n",
    "(combining and adapting materials/discussion evolved over time by Huihuo Zheng, Bethany Lusch, Asad Khan, Prasanna Balaprakash, Taylor Childers, Corey Adams, Kyle Felker, Varuni Sastry, Sam Foreman, Archit Vasan, Carlo Graziani, Tanwi Mallick, and Venkat Vishwanath)\n",
    "## Outline \n",
    "1. Day 1\n",
    "    - Evolution of computig systems\n",
    "    - Parallel computing\n",
    "    - Introduction to Deep Learning\n",
    "    - ***Parallel computing in AI***\n",
    "\n",
    "\n",
    "2. ***Day 2***\n",
    "    - ***Parallel computing in AI***\n",
    "    - Brief Introduction to LLMs\n",
    "    - Hands-on LLM training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Computing in AI \n",
    "### Recap Single GPU\n",
    "![x0](images/mermaid-figure-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed training is the process of training models across multiple GPUs or other accelerators, with the goal of speeding up the training process and enabling the training of larger models on larger datasets.\n",
    "\n",
    "There are two ways of parallelization in distributed training. \n",
    "* ***Data parallelism***: \n",
    "    * Each worker (GPU) has a complete set of model\n",
    "    * different workers work on different subsets of data. \n",
    "* *Model parallelism* \n",
    "    * The model is split into different parts and stored on different workers\n",
    "    * Different workers work on computation involved in different parts of the model\n",
    "![PI](images/parallel_computing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling goal: \n",
    "1. Minimize cost i.e. amount of time spent training\n",
    "2. Maximize performance i.e model quality metrics, throughput/efficiency metrics (images/seconds, GPU/CPU utilization percentages, flops efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on multiple GPUs: Data Parallelism\n",
    "### Nomenclature: \n",
    "- N = number of GPUs = WORLD_SIZE\n",
    "- Each GPU is assigned a rank from 0 to WORLD_SIZE-1\n",
    "- A worker = a GPU here\n",
    "![mgpus](images/mermaid-figure-15.png)\n",
    "*Each GPU receives unique data at each step*\n",
    "### Data Parallel: Forward Pass\n",
    "![forward](images/mermaid-figure-14.png)\n",
    "*Average gradients across all GPUs*\n",
    "### Data Parallel: Backward Pass\n",
    "![backward](images/mermaid-figure-13.png)\n",
    "*Send global updates back to each GPU*\n",
    "### Data Parallel: Full Setup\n",
    "![full](images/mermaid-figure-12.png)\n",
    "*See: [PyTorch / Distributed Data Parallel](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html)*\n",
    "### Data Parallel: Training\n",
    "- Each GPU:\n",
    "    - has identical copy of model\n",
    "    - works on a unique subset of data\n",
    "- Easy to get started with\n",
    "    - [saforeman2/ezpz](https://github.com/saforem2/ezpz)\n",
    "    - [PyTorch/DDP](https://docs.pytorch.org/docs/stable/notes/ddp.html)\n",
    "    - [HF/Accelerate](https://huggingface.co/docs/transformers/accelerate)\n",
    "    - [Microsoft/DeepSpeed](https://www.deepspeed.ai)\n",
    "- Requires ***global*** communication\n",
    "    - every rank must participate (collective communication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication\n",
    "- Need mechanism(s) for communicating across GPUs:\n",
    "    - [mpi4py](https://mpi4py.readthedocs.io/en/stable/tutorial.html)\n",
    "    - [torch.distributed](https://docs.pytorch.org/docs/stable/distributed.html)\n",
    "- Collective communication:\n",
    "    - [Nvidia Collective Communications Library (NCCL)](https://developer.nvidia.com/nccl)\n",
    "    - [Intel oneAPI Collective Communications](https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html#gs.n9y302)\n",
    "***Timeouts*** Collective operations have to be called for each rank to form a complete collective operation.\n",
    "Failure to do so will result in other ranks waiting indefinitely\n",
    "### AllReduce\n",
    "![allreduce](images/mermaid-figure-11.png)\n",
    "\n",
    "### Broadcast\n",
    "![broadcast](images/mermaid-figure-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Data\n",
    "- At each training step, we want to ensure that each worker receives unique data\n",
    "- This can be done in one of two ways:\n",
    "    1. Manually partition data (ahead of time)\n",
    "        - Assign unique subsets to each worker\n",
    "        - Each worker can only see their local portion of the data\n",
    "        - Most common approach\n",
    "    2. From each worker, randomly select a mini-batch\n",
    "        - Each worker can see the full dataset\n",
    "        - ⚠️ When randomly selecting, it is important that each worker uses different seeds to ensure they receive unique data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Initial State\n",
    "- At the start of training (or when loading from a checkpoint), we want all of our workers to be initialized consistently\n",
    "    - Broadcast the model and optimizer states from rank() == 0 worker\n",
    "![bcast](images/mermaid-figure-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why distributed training?\n",
    "- N workers each processing unique batch (micro batch size) of data:\n",
    "    - (micro_batch_size = 1)× $N_{GPUs}$ → ***global_batch_size = N***\n",
    "- Improved gradient estimators\n",
    "    - Smooth loss landscape\n",
    "    - Less iterations needed for same number of epochs\n",
    "        - common to scale learning rate lr *= sqrt(N)\n",
    "        \n",
    "![speedup](images/speedup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Beyond Data Parallelism\n",
    "- Useful when model fits on single GPU:\n",
    "    - ultimately limited by GPU memory\n",
    "    - model performance limited by size\n",
    "- ⚠️ When model does not fit on a single GPU:\n",
    "    - Offloading (can only get you so far…):\n",
    "        - DeepSpeed + ZeRO\n",
    "        - PyTorch + FSDP\n",
    "- Otherwise, resort to [model parallelism strategies](https://samforeman.me/talks/ai-for-science-2024/slides#/additional-parallelism-strategies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going beyond Data Parallelism:  DeepSpeed + ZeRO\n",
    "- Depending on the ZeRO stage (1, 2, 3), we can offload\n",
    "    1. ***Stage 1***: optimizer states (P_{os})\n",
    "    2. ***Stage 2***: optimizer states+gradients (P_{os+g})\n",
    "    2. ***Stage 3***: optimizer states+gradients+model params (P_{os+g+p})\n",
    "\n",
    "![zero](images/zero.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parallel training: example\n",
    "Want to compute $y = \\sum_i x_iW_i = x_0W_0 + x_1W_1 + x_2W_2$ where each GPU only has only its portion of the full weights as shown below\n",
    "1. Compute $y_0=x0W_0$ -> **GPU1**\n",
    "2. Compute $y_1=y_0 +x_1W_1$ -> **GPU2**\n",
    "3. Compute $y_2=y_1 + x_2W_2$\n",
    "\n",
    "![modelpar](images/mermaid-figure-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding on a parallelism strategy\n",
    "![onedec](images/onegpudec.png)\n",
    "![multgpu](images/onenodemulgpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AIcompute](images/ai-and-compute-all-2.png.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sophia: 192 GPUs (8/node), 3.9 Petaflops ($10^15$)/s\n",
    "![sophia](images/sophia.jpeg)\n",
    "Polaris: 2240 GPUs (4/node), 78 Teraflops ($10^12$)/s\n",
    "![polaris](images/polaris.jpeg)\n",
    "Aurora: 63,744 GPUs (6/node), exascale computer ($10^18$ calculations per second)\n",
    "![aurora](images/aurora.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief introduction to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Life-cycle of a LLM\n",
    "1. Data collection + preprocessing\n",
    "2. ***Pre-training***\n",
    "    - Architecture decisions, model size, etc.\n",
    "3. Supervised Fine-Tuning\n",
    "    - Instruction Tuning\n",
    "    - Alignment\n",
    "4. Deploy (+ monitor, re-evaluate, etc.)\n",
    "\n",
    "![gptcycle](images/gpt3-training-step-back-prop.gif)\n",
    "*Source:Figure from [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Life-cycle of a LLM\n",
    "1. Data collection + preprocessing\n",
    "2. Pre-training\n",
    "    - Architecture decisions, model size, etc.\n",
    "3. ***Supervised Fine-Tuning***\n",
    "    - Instruction Tuning\n",
    "    - Alignment\n",
    "4. Deploy (+ monitor, re-evaluate, etc.)\n",
    "\n",
    "![gptcycle2](images/gpt3-fine-tuning.gif)\n",
    "*Source:Figure from [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "![fwdpass](images/hf_assisted_generation.mov)\n",
    "*Source: [Generation with LLMs](https://huggingface.co/docs/transformers/main/en/llm_tutorial)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text\n",
    "![fwdpass](images/hf_assisted_generation2.mov)\n",
    "*Source: [Generation with LLMs](https://huggingface.co/docs/transformers/main/en/llm_tutorial)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on LLM Training\n",
    "\n",
    "\n",
    "***Good practice*** (not needed here): Create and activate a conda (or virtual) environment \n",
    "```conda create -n env_mlss_dnn python=3.9```\n",
    "then on jupyter do new ->terminal\n",
    "\n",
    "```\n",
    " conda activate env_mlss_dnn\n",
    " pip install ipykernel \n",
    " python -m ipykernel install --user --name env_mlss_dnn\n",
    "```\n",
    "\n",
    "then go back to your .ipynb file, change kernel to env_mlss_dnn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/karpathy/nanoGPT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "\n",
    "%cd nanoGPT\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset               | Tokens (≈)         | Disk size / notes                                 |\n",
    "| --------------------- | ------------------ | ------------------------------------------------- |\n",
    "| **openwebtext**       | 9 B tokens total   | 9 B train (≈17GB) / 4M val (≈8.5MB)                |\n",
    "| **shakespeare (tiny)** | ≈ 330K tokens total | 301,966 train / 36,059 val                   |\n",
    "| **shakespeare\\_char** | 1,115,394 chars    | 1,003,854 train / 111,540 val (character‐level)   |\n",
    "\n",
    "\n",
    "| model | params | train loss | val loss |\n",
    "| ------| ------ | ---------- | -------- |\n",
    "| gpt2 | 124M         | 3.11  | 3.12     |\n",
    "| gpt2-medium | 350M  | 2.85  | 2.84     |\n",
    "| gpt2-large | 774M   | 2.66  | 2.67     |\n",
    "| gpt2-xl | 1558M     | 2.56  | 2.54     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/Resources/Python.app/Contents/MacOS/Python: can't open file 'data/shakespeare_char/prepare.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!python3 data/shakespeare_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train.py config/train_shakespeare_char.py --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=16\n",
    "--n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 sample.py --out_dir=out-shakespeare-char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train.py config/train_shakespeare_char.py #training longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 sample.py --out_dir=out-shakespeare-char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export NCCL_DEBUG=INFO\n",
    "!export NCCL_DEBUG_SUBSYS=ALL\n",
    "!export NCCL_DEBUG_FILE=nccl_trace.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on NVIDIA T4 Tensor Cores, 4GPUS/node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "ip=socket.gethostbyname(socket.gethostname())\n",
    "print(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0 #,1,2,3          \n",
    "!export MASTER_ADDR=ip           \n",
    "!export MASTER_PORT=29500                     \n",
    "\n",
    "!torchrun \\\n",
    "  --nnodes=1 \\\n",
    "  --node_rank=0 \\\n",
    "  --nproc_per_node=12 \\\n",
    "  --master_addr=$ip \\\n",
    "  --master_port=29500 \\\n",
    "  train.py \\\n",
    "    config/train_shakespeare_char.py \\\n",
    "    --batch_size=64 \\\n",
    "    --gradient_accumulation_steps=40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![llms](images/llms.gif)\n",
    "*Source: [Hannibal046/Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![emergent](images/emergent-abilities.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![evolllms](images/evolution.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZV5UwakJTbqgB0ItPwe9k",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_mlss_dnn",
   "language": "python",
   "name": "env_mlss_dnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
