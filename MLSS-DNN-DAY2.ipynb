{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/IntroLLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel and distributed Deep Learning\n",
    "\n",
    "## Author: Marieme Ngom, Argonne National Laboratory\n",
    "(combining and adapting materials/discussion evolved over time by Huihuo Zheng, Bethany Lusch, Asad Khan, Prasanna Balaprakash, Taylor Childers, Corey Adams, Kyle Felker, Varuni Sastry, Sam Foreman, Archit Vasan, Carlo Graziani, Tanwi Mallick, and Venkat Vishwanath)\n",
    "## Outline \n",
    "1. Day 1\n",
    "    - Evolution of computig systems\n",
    "    - Parallel computing\n",
    "    - Introduction to Deep Learning\n",
    "    - Parallel computing in AI\n",
    "\n",
    "\n",
    "2. ***Day 2***\n",
    "    - Brief Introduction to LLMs\n",
    "    - Hands-on LLM training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief introduction to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![llms](images/llms.gif)\n",
    "*Source: [Hannibal046/Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![emergent](images/emergent-abilities.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LLMs\n",
    "![evolllms](images/evolution.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ithungers](images/it_hungers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Life-cycle of a LLM\n",
    "1. Data collection + preprocessing\n",
    "2. ***Pre-training***\n",
    "    - Architecture decisions, model size, etc.\n",
    "3. Supervised Fine-Tuning\n",
    "    - Instruction Tuning\n",
    "    - Alignment\n",
    "4. Deploy (+ monitor, re-evaluate, etc.)\n",
    "\n",
    "![gptcycle](images/gpt3-training-step-back-prop.gif)\n",
    "*Source:Figure from [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Life-cycle of a LLM\n",
    "1. Data collection + preprocessing\n",
    "2. Pre-training\n",
    "    - Architecture decisions, model size, etc.\n",
    "3. ***Supervised Fine-Tuning***\n",
    "    - Instruction Tuning\n",
    "    - Alignment\n",
    "4. Deploy (+ monitor, re-evaluate, etc.)\n",
    "\n",
    "![gptcycle](images/gpt3-fine-tuning.gif)\n",
    "*Source:Figure from [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "![fwdpass](images/hf_assisted_generation.mov)\n",
    "*Source: [Generation with LLMs](https://huggingface.co/docs/transformers/main/en/llm_tutorial)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text\n",
    "![fwdpass](images/hf_assisted_generation2.mov)\n",
    "*Source: [Generation with LLMs](https://huggingface.co/docs/transformers/main/en/llm_tutorial)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on LLM Training\n",
    "\n",
    "\n",
    "\n",
    "***Good practice***: Create and activate a conda (or virtual) environment \n",
    "```conda create -n env_mlss_dnn python=3.9```\n",
    "then on jupyter do new ->terminal\n",
    "\n",
    "```\n",
    " conda activate env_mlss_dnn\n",
    " pip install ipykernel \n",
    " python -m ipykernel install --user --name env_mlss_dnn\n",
    "```\n",
    "\n",
    "then go back to your .ipynb file, change kernel to env_mlss_dnn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/karpathy/nanoGPT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "\n",
    "# change to /path/to/your/folder\n",
    "%cd nanoGPT\n",
    "\n",
    "# confirm\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 data/shakespeare_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train.py config/train_shakespeare_char.py --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=500 --lr_decay_iters=2000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 sample.py --out_dir=out-shakespeare-char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export NCCL_DEBUG=INFO\n",
    "!export NCCL_DEBUG_SUBSYS=ALL\n",
    "!export NCCL_DEBUG_FILE=nccl_trace.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on one NVIDIA T4 Tensor Cores, 4GPUS/node\n",
    "# Bamba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "\n",
    "# launch 4 processes â†’ 4 GPUs\n",
    "!torchrun \\\n",
    "  --nproc_per_node=4 \\\n",
    "  train.py \\\n",
    "    config/train_shakespeare_char.py \\\n",
    "    --batch_size=12 \\\n",
    "    --gradient_accumulation_steps=40 \\\n",
    "    --compile=False 2>&1 | tee full_train.log\n",
    "\n",
    "#I am on my personal laptop hence the following error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on N NVIDIA T4 Tensor Cores (4N GPUs), Because each participant has on ly one node we set --nnodes=1.  With multiple nodes, you need to do the following on each node:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "ip=socket.gethostbyname(socket.gethostname())\n",
    "print(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1,2,3          \n",
    "!export MASTER_ADDR=ip           \n",
    "!export MASTER_PORT=29500                     \n",
    "\n",
    "!torchrun \\\n",
    "  --nnodes=1 \\\n",
    "  --node_rank=0 \\\n",
    "  --nproc_per_node=4 \\\n",
    "  --master_addr=$ip \\\n",
    "  --master_port=29500 \\\n",
    "  train.py \\\n",
    "    config/train_shakespeare_char.py \\\n",
    "    --batch_size=12 \\\n",
    "    --gradient_accumulation_steps=40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZV5UwakJTbqgB0ItPwe9k",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_mlss_dnn",
   "language": "python",
   "name": "env_mlss_dnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
