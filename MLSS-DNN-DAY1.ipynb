{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/IntroLLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel and distributed Deep Learning\n",
    "\n",
    "## Author: Marieme Ngom, Argonne National Laboratory\n",
    "(combining and adapting materials/discussion evolved over time by Huihuo Zheng, Bethany Lusch, Asad Khan, Prasanna Balaprakash, Taylor Childers, Corey Adams, Kyle Felker, Varuni Sastry, Sam Foreman, Archit Vasan, Carlo Graziani, Tanwi Mallick, and Venkat Vishwanath)\n",
    "## Outline \n",
    "1. Day 1\n",
    "    - Evolution of computig systems\n",
    "    - Parallel computing\n",
    "    - Introduction to Deep Learning\n",
    "    - Parallel computing in AI\n",
    "\n",
    "\n",
    "2. Day 2\n",
    "    - Brief Introduction to LLMs\n",
    "    - Hands-on LLM training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel computing\n",
    "**Parallel computing** refers to the process of breaking down larger problems into smaller, independent, often similar parts that can be executed simultaneously by multiple processors communicating via network or shared memory, the results of which are combined upon completion as part of an overall algorithm.\n",
    "\n",
    "## $\\pi$ example\n",
    "We can calculate the value of $\\pi$ using a MPI parallelized version of the Monte Carlo method. The basic idea is to estimate $\\pi$ by randomly sampling points within a square and determining how many fall inside a quarter circle inscribed within that square.\n",
    "\n",
    "![PI](https://www.101computing.net/wp/wp-content/uploads/estimating-pi-monte-carlo-method.png)\n",
    "\n",
    "The ratio between the area of the circle and the square is\n",
    "\n",
    "$\\frac{N_\\text{in}}{N_\\text{total}} = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4}$\n",
    "\n",
    "Therefore, we can calculate $\\pi$ using \n",
    "$\\pi = \\frac{4N_\\text{in}}{N_\\text{total}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "fig, ax = plt.subplots()\n",
    "#ax = fig.add_subplot(111)\n",
    "circle = plt.Circle(( 0. , 0. ), 0.5 )\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.ylim(-0.5, 0.5)\n",
    "ax.add_patch(circle)\n",
    "ax.set_aspect('equal')\n",
    "N = 500\n",
    "Nin = 0\n",
    "t0 = time.time()\n",
    "for i in range(1, N+1):\n",
    "    x = random.uniform(-0.5, 0.5)\n",
    "    y = random.uniform(-0.5, 0.5)\n",
    "    if (np.sqrt(x*x + y*y) < 0.5):\n",
    "        Nin += 1\n",
    "        plt.plot([x], [y], 'o', color='r', markersize=3)\n",
    "    else:\n",
    "        plt.plot([x], [y], 'o', color='b', markersize=3)\n",
    "    display(fig)\n",
    "    plt.xlabel(\"$\\pi$ = %3.4f \\n N_in / N_total = %5d/%5d\" %(Nin*4.0/i, Nin, i))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "res = np.array(Nin, dtype='d')\n",
    "t1 = time.time()\n",
    "print(f\"Pi = {res/float(N/4.0)}\")\n",
    "print(\"Time: %s\" %(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI example\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "N = 5000000\n",
    "Nin = 0\n",
    "t0 = time.time()\n",
    "for i in range(comm.rank, N, comm.size):\n",
    "    x = random.uniform(-0.5, 0.5)\n",
    "    y = random.uniform(-0.5, 0.5)\n",
    "    if (np.sqrt(x*x + y*y) < 0.5):\n",
    "        Nin += 1\n",
    "res = np.array(Nin, dtype='d')\n",
    "res_tot = np.array(Nin, dtype='d')\n",
    "comm.Allreduce(res, res_tot, op=MPI.SUM)\n",
    "t1 = time.time()\n",
    "if comm.rank==0:\n",
    "    print(res_tot/float(N/4.0))\n",
    "    print(\"Time: %s\" %(t1 - t0))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wget\n",
    "\n",
    "# # Download a file\n",
    "# url = \"https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/01_intro_AI_on_Supercomputer/mpi_pi.py\"\n",
    "# filename = wget.download(url)\n",
    "# print(f\"Downloaded: {filename}\")\n",
    "\n",
    "# # With custom filename\n",
    "# # wget.download(url, \"custom_name.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comment the next line if not needed for your system \n",
    "!sudo apt-get install mpich -y\n",
    "\n",
    "!pip install mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mpirun -np 1 python3 mpi_pi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mpirun -np 2 python3 mpi_pi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mpirun -np 4 python3 mpi_pi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will serve as a gentle introduction to neural networks and deep learning through a hands-on classification problem using the MNIST dataset. \n",
    "\n",
    "In particular, we will introduce neural networks and how to train and improve their learning capabilities.  We will use the PyTorch Python library.\n",
    "\n",
    "The [MNIST dataset](http://yann.lecun.com/exdb/mnist/) contains thousands of examples of handwritten numbers, with each digit labeled 0-9.\n",
    "<img src=\"images/mnist_task.png\"  align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "We will now download the dataset that contains handwritten digits. MNIST is a popular dataset, so we can download it via the PyTorch library. Note:\n",
    "- x is for the inputs (images of handwritten digits) and y is for the labels or outputs (digits 0-9)\n",
    "- We are given \"training\" and \"test\" datasets. Training datasets are used to fit the model. Test datasets are saved until the end, when we are satisfied with our model, to estimate how well our model generalizes to new data.\n",
    "\n",
    "Note that downloading it the first time might take some time.\n",
    "The data is split as follows:\n",
    "- 60,000 training examples, 10,000 test examples\n",
    "- inputs: 1 x 28 x 28 pixels\n",
    "- outputs (labels): one integer per example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "val_size = len(training_data) - train_size  # Remaining 20% for validation\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=torch.Generator().manual_seed(55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MNIST data loaded: train:',len(training_data),' examples, validation: ', len(validation_data), 'examples, test:',len(test_data), 'examples')\n",
    "print('Input shape', training_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look. Here are the first 10 training digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(numpy.reshape(training_data[i][0], (28, 28)), cmap=\"gray\")\n",
    "    plt.title('Class: '+str(training_data[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalities:\n",
    "To train our classifier, we need (besides the data):\n",
    "- A model that depend on parameters $\\mathbf{\\theta}$. Here we are going to use neural networks.\n",
    "- A loss function $J(\\mathbf{\\theta})$ to measure the capabilities of the model.\n",
    "- An optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model\n",
    "Let's begin with a simple linear model: linear regression. \n",
    "We add one complication: each example is a vector (flattened image), so the \"slope\" multiplication becomes a dot product. If the target output is a vector as well, then the multiplication becomes matrix multiplication. \n",
    "\n",
    "Note, like before, we consider multiple examples at once, adding another dimension to the input. \n",
    "\n",
    "\n",
    " <img src=\"images/LinearModel_1.png\"  align=\"center\"/>\n",
    " \n",
    "\n",
    "\n",
    "The linear layers in PyTorch perform a basic $xW + b$. These \"fully connected\" layers connect each input to each output with some weight parameter. We wouldn't expect a simple linear model $f(x) = xW+b$ directly outputting the class label and minimizing mean squared error to work well - the model would output labels like 3.55 and 2.11 instead of skipping to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need:\n",
    "- A loss function $J(\\theta)$ where $\\theta$ is the list of parameters (here W and b).\n",
    "    - Change the output to be a length-10 vector of class probabilities (0 to 1, adding to 1).\n",
    "    - Cross entropy as the loss function, which is typical for classification. You can read more [here](https://gombru.github.io/2018/05/23/cross_entropy_loss/). \n",
    "\n",
    "- An optimization method or optimizer such as the stochastic gradient descent (sgd) method, the Adam optimizer, RMSprop, Adagrad etc. Let's start with stochastic gradient descent (sgd). For far more information about more advanced optimizers than basic SGD, with some cool animations, see https://ruder.io/optimizing-gradient-descent/ or https://distill.pub/2017/momentum/.\n",
    "\n",
    "- A learning rate which controls how far we move during each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First, we need to convert the input image to a vector by using \n",
    "        # nn.Flatten(). For MNIST, it means the second dimension 28*28 becomes 784.\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Here, we add a fully connected (\"dense\") layer that has 28 x 28 = 784 input nodes \n",
    "        #(one for each pixel in the input image) and 10 output nodes (for probabilities of each class).\n",
    "        self.layer_1 = nn.Linear(28*28, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer_1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearClassifier()\n",
    "print(linear_model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(linear_model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "Now we are ready to train our first model. A training step is comprised of:\n",
    "- A forward pass: the input is passed through the network\n",
    "- Backpropagation: A backward pass to compute the gradient $\\frac{\\partial J}{\\partial \\mathbf{W}}$ of the loss function with respect to the parameters of the network.\n",
    "- Weight updates $\\mathbf{W} = \\mathbf{W} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W}} $ where $\\alpha$ is the learning rate.\n",
    "\n",
    "How many steps do we take?\n",
    "- The batch size corresponds to the number of training examples in one pass (forward + backward). A smaller batch size allows the model to learn from individual examples but takes longer to train. A larger batch size requires fewer steps but may result in the model not capturing the nuances in the data. The higher the batch size, the more memory you will require.  \n",
    "- An epoch means one pass through the whole training data (looping over the batches). Using few epochs can lead to underfitting and using too many can lead to overfitting.\n",
    "- The choice of batch size and learning rate are important for performance, generalization and accuracy in deep learning.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "train_acc_all = []\n",
    "val_acc_all = []\n",
    "for j in range(epochs):\n",
    "    train_one_epoch(train_dataloader, linear_model, loss_fn, optimizer)\n",
    "    \n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "    acc, loss = evaluate(train_dataloader, linear_model, loss_fn)\n",
    "    train_acc_all.append(acc)\n",
    "    print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "    \n",
    "    # checking on the validation loss and accuracy once per epoch\n",
    "    val_acc, val_loss = evaluate(val_dataloader, linear_model, loss_fn)\n",
    "    val_acc_all.append(val_acc)\n",
    "    print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, 10 * pltsize))\n",
    "plt.plot(range(epochs), train_acc_all,label = 'Training Acc.' )\n",
    "plt.plot(range(epochs), val_acc_all, label = 'Validation Acc.' )\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how the model is doing on the first 10 examples\n",
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "linear_model.eval()\n",
    "batch = next(iter(train_dataloader))\n",
    "predictions = linear_model(batch[0])\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(batch[0][i,0,:,:], cmap=\"gray\")\n",
    "    plt.title('%d' % predictions[i,:].argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: How can you improve the accuracy? Some things you might consider: increasing the number of epochs, changing the learning rate, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Let's see how our model generalizes to the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_test = 256 \n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test, loss_test = evaluate(test_dataloader, linear_model, loss_fn)\n",
    "print(\"Test loss: %.4f, test accuracy: %.2f%%\" % (loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a closer look at the results.\n",
    "\n",
    "Let's define a helper function to show the failure cases of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_failures(model, dataloader, maxtoshow=10):\n",
    "    model.eval()\n",
    "    batch = next(iter(dataloader))\n",
    "    predictions = model(batch[0])\n",
    "    \n",
    "    rounded = predictions.argmax(1)\n",
    "    errors = rounded!=batch[1]\n",
    "    print('Showing max', maxtoshow, 'first failures. '\n",
    "          'The predicted class is shown first and the correct class in parentheses.')\n",
    "    \n",
    "    # Create all subplots at once\n",
    "    fig, axes = plt.subplots(1, maxtoshow, figsize=(maxtoshow, 1))\n",
    "    if maxtoshow == 1:\n",
    "        axes = [axes]  # Handle single subplot case\n",
    "    \n",
    "    ii = 0\n",
    "    for i in range(batch[0].shape[0]):\n",
    "        if ii >= maxtoshow:\n",
    "            break\n",
    "        if errors[i]:\n",
    "            axes[ii].axis('off')\n",
    "            axes[ii].imshow(batch[0][i,0,:,:], cmap=\"gray\")\n",
    "            axes[ii].set_title(\"%d (%d)\" % (rounded[i], batch[1][i]))\n",
    "            ii += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 images from the test data that this small model classified to a wrong class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_failures(linear_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Model\n",
    "Our linear model isn't enough for high accuracy on this dataset. To improve the model, we often need to add more layers and nonlinearities.\n",
    "<img src=\"images/shallow_nn.png\"  align=\"left\"/>\n",
    "\n",
    "The output of this NN can be written as\n",
    "\\begin{equation}\\label{eq: NN1d}\n",
    "  \\hat{u}(x) = \\sigma_2(\\sigma_1(\\mathbf{x}\\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2),\n",
    "\\end{equation}\n",
    "where $\\mathbf{x}$ is the input, $\\mathbf{W}_j$ are the weights of the neural network, $\\sigma_j$ the (nonlinear) activation functions, and $\\mathbf{b}_j$ its biases. The activation function introduces the nonlinearity and makes it possible to learn more complex tasks. Desirable properties in an activation function include being differentiable, bounded, and monotonic.\n",
    "\n",
    "\n",
    "Image source: [PragatiBaheti](https://www.v7labs.com/blog/neural-networks-activation-functions)\n",
    "<img src=\"images/activation.jpeg\"  align=\"center\"/>\n",
    "\n",
    "Adding more layers to obtain a deep neural network:\n",
    "<img src=\"images/deep_nn_annotated.jpg\"  align=\"left\"/>\n",
    "\n",
    "\n",
    "\n",
    "# Important things to know\n",
    "Deep Neural networks can be overly flexible/complicated and \"overfit\" your data, just like fitting overly complicated polynomials:\n",
    "<img src=\"images/bias_vs_variance.png\"  align=\"left\"/>\n",
    "\n",
    "\n",
    "Vizualization wrt to the accuracy and loss (Image source: [Baeldung](https://www.baeldung.com/cs/ml-underfitting-overfitting)):\n",
    "<img src=\"images/acc_under_over.webp\"  align=\"center\"/>\n",
    "\n",
    "\n",
    "To improve the generalization of our model on previously unseen data, we employ a technique known as regularization, which constrains our optimization problem in order to discourage complex models.\n",
    "\n",
    "  - Dropout is the commonly used regularization technique. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "  - Penalizing the loss function by adding a term such as $\\lambda ||\\mathbf{W}||^2$ is alsp a commonly used regularization technique. This helps \"control\" the magnitude of the weights of the network.\n",
    "    \n",
    "Vanishing gradients: Gradients become small as they propagate backward through the layers. Squashing activation functions like sigmoid or tanh could cause this.\n",
    "\n",
    "Exploding gradients: Gradients grow exponentially usually due to \"poor\" weight initialization.\n",
    "\n",
    "<!--  <img src=\"images/test_data_rule.png\" width=\"800\" hight=\"500\" align=\"center\"/>\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement a deep network in PyTorch. nn.Dropout() performs the Dropout operation mentioned earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonlinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "           # nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "           # nn.Dropout(0.2),\n",
    "            nn.Linear(50, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers_stack(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_model = NonlinearClassifier()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "train_acc_all = []\n",
    "val_acc_all = []\n",
    "for j in range(epochs):\n",
    "    train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n",
    "    \n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "    acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n",
    "    train_acc_all.append(acc)\n",
    "    print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "    \n",
    "    # checking on the validation loss and accuracy once per epoch\n",
    "    val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n",
    "    val_acc_all.append(val_acc)\n",
    "    print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, 10 * pltsize))\n",
    "plt.plot(range(epochs), train_acc_all,label = 'Training Acc.' )\n",
    "plt.plot(range(epochs), val_acc_all, label = 'Validation Acc.' )\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_failures(nonlinear_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\n",
    "#    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# targetting the first GPU cuba:0\n",
    "device = torch.device(\n",
    "    \"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "nonlinear_model = NonlinearClassifier().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_device(dataloader, model, loss_fn, device):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "            pred = model.to(device)(X.to(device))\n",
    "            loss += loss_fn(pred, y.to(device)).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_device(dataloader, model, loss_fn, optimizer,device):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X.to(device))\n",
    "        loss = loss_fn(pred, y.to(device))\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "train_acc_all = []\n",
    "val_acc_all = []\n",
    "for j in range(epochs):\n",
    "    train_one_epoch_device(train_dataloader, nonlinear_model, loss_fn, optimizer,device) # modified this to use the nonlinear_model instead\n",
    "    \n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "    acc, loss = evaluate_device(train_dataloader, nonlinear_model, loss_fn)  # modified this to use the  evaluate_device function and the nonlinear_model instead\n",
    "    train_acc_all.append(acc)\n",
    "    print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "    \n",
    "    # checking on the validation loss and accuracy once per epoch\n",
    "    val_acc, val_loss = evaluate_device(val_dataloader, nonlinear_model, loss_fn) # modified this to use the  evaluate_device function and the nonlinear_model instead\n",
    "    val_acc_all.append(val_acc)\n",
    "    print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Computing in AI \n",
    "evolved from Sam Foreman and Huihuo Zheng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed training is the process of training I models across multiple GPUs or other accelerators, with the goal of speeding up the training process and enabling the training of larger models on larger datasets.\n",
    "\n",
    "There are two ways of parallelization in distributed training. \n",
    "* **Data parallelism**: \n",
    "    * Each worker (GPU) has a complete set of model\n",
    "    * different workers work on different subsets of data. \n",
    "* **Model parallelism** \n",
    "    * The model is split into different parts and stored on different workers\n",
    "    * Different workers work on computation involved in different parts of the model\n",
    "![PI](images/parallel_computing.png)\n",
    "![3D LLM](images/3DLLM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling goal: \n",
    "1. Minimize cost i.e. amount of time spent training\n",
    "2. Maximize performance i.e "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on a single device\n",
    "\n",
    "## Single GPU\n",
    "![x0](images/mermaid-figure-1.png)\n",
    "![x1](images/mermaid-figure-17.png)\n",
    "![x2](images/mermaid-figure-16.png)\n",
    "\n",
    "## Recipes for efficient training on a single GPU\n",
    "\n",
    "***Batch size choice***\n",
    "\n",
    "***Gradient accumulatiom***\n",
    "\n",
    "***Mixed precision training***\n",
    "\n",
    "\n",
    "## More in [Methods and tools for efficient training on a single GPU](https://huggingface.co/docs/transformers/v4.46.0/perf_train_gpu_one)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on multiple GPUs: Data Parallelism\n",
    "![mgpus](images/mermaid-figure-15.png)\n",
    "*Each GPU receives unique data at each step*\n",
    "## Data Parallel: Forward Pass\n",
    "![forward](images/mermaid-figure-14.png)\n",
    "*Average gradients across all GPUs*\n",
    "## Data Parallel: Backward Pass\n",
    "![backward](images/mermaid-figure-13.png)\n",
    "*Send global updates back to each GPU*\n",
    "## Data Parallel: Full Setup\n",
    "![full](images/mermaid-figure-12.png)\n",
    "*See: [PyTorch / Distributed Data Parallel](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html)*\n",
    "## Data Parallel: Training\n",
    "- Each GPU:\n",
    "    - has identical copy of model\n",
    "    - works on a unique subset of data\n",
    "- Easy to get started with\n",
    "    - [saforeman2/ezpz](https://github.com/saforem2/ezpz)\n",
    "    - [PyTorch/DDP](https://docs.pytorch.org/docs/stable/notes/ddp.html)\n",
    "    - [HF/Accelerate](https://huggingface.co/docs/transformers/accelerate)\n",
    "    - [Microsoft/DeepSpeed](https://www.deepspeed.ai)\n",
    "- Requires ***global*** communication\n",
    "    - every rank must participate (collective communication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communication\n",
    "- Need mechanism(s) for communicating across GPUs:\n",
    "    - [mpi4py](https://mpi4py.readthedocs.io/en/stable/tutorial.html)\n",
    "    - [torch.distributed](https://docs.pytorch.org/docs/stable/distributed.html)\n",
    "- Collective communication:\n",
    "    - [Nvidia Collective Communications Library (NCCL)](https://developer.nvidia.com/nccl)\n",
    "    - [Intel oneAPI Collective Communications](https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html#gs.n9y302)\n",
    "***Timeouts*** Collective operations have to be called for each rank to form a complete collective operation.\n",
    "Failure to do so will result in other ranks waiting indefinitely\n",
    "## AllReduce\n",
    "![allreduce](images/mermaid-figure-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why distributed training?\n",
    "- N workers each processing unique batch (micro batch size) of data:\n",
    "    - (micro_batch_size = 1)× $N_{GPUs}$ → ***global_batch_size = N***\n",
    "- Improved gradient estimators\n",
    "    - Smooth loss landscape\n",
    "    - Less iterations needed for same number of epochs\n",
    "        - common to scale learning rate lr *= sqrt(N)\n",
    "        \n",
    "![speedup](images/speedup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Data\n",
    "- At each training step, we want to ensure that each worker receives unique data\n",
    "- This can be done in one of two ways:\n",
    "    1. Manually partition data (ahead of time)\n",
    "        - Assign unique subsets to each worker\n",
    "        - Each worker can only see their local portion of the data\n",
    "        - Most common approach\n",
    "    2. From each worker, randomly select a mini-batch\n",
    "        - Each worker can see the full dataset\n",
    "        - ⚠️ When randomly selecting, it is important that each worker uses different seeds to ensure they receive unique data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcast Initial State\n",
    "- At the start of training (or when loading from a checkpoint), we want all of our workers to be initialized consistently\n",
    "    - Broadcast the model and optimizer states from rank() == 0 worker\n",
    "![bcast](images/mermaid-figure-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices\n",
    "**Keeping things in Sync**\n",
    "***Computation stalls during communication !!***\n",
    "\n",
    "Keeping the communication to computation ratio small is important for effective scaling.\n",
    "\n",
    "- Use parallel IO whenever possible\n",
    "    - Feed each rank from different files\n",
    "    - Use MPI IO to have each rank read its own batch from a file\n",
    "    - Use several ranks to read data, MPI to scatter to remaining ranks\n",
    "        - Most practical in big at-scale training\n",
    "- Take advantage of data storage\n",
    "    - Use striping on lustre\n",
    "    - Use the right optimizations for your HPC cluster.\n",
    "    - Preload data when possible\n",
    "    - Offloading to a GPU frees CPU cycles for loading the next batch of data\n",
    "        - minimize IO latency this way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Going Beyond Data Parallelism\n",
    "- Useful when model fits on single GPU:\n",
    "    - ultimately limited by GPU memory\n",
    "    - model performance limited by size\n",
    "- ⚠️ When model does not fit on a single GPU:\n",
    "    - Offloading (can only get you so far…):\n",
    "        - DeepSpeed + ZeRO\n",
    "        - PyTorch + FSDP\n",
    "- Otherwise, resort to [model parallelism strategies](https://samforeman.me/talks/ai-for-science-2024/slides#/additional-parallelism-strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ishneA15OLNS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZV5UwakJTbqgB0ItPwe9k",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
