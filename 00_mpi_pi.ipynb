{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3595278f-6b75-4c99-a54c-0d3d47cb7f57",
   "metadata": {},
   "source": [
    "# Parallel computing\n",
    "**Parallel computing** refers to the process of breaking down larger problems into smaller, independent, often similar parts that can be executed simultaneously by multiple processors communicating via network or shared memory, the results of which are combined upon completion as part of an overall algorithm.\n",
    "\n",
    "## $\\pi$ example\n",
    "We can calculate the value of $\\pi$ using a MPI parallelized version of the Monte Carlo method. The basic idea is to estimate $\\pi$ by randomly sampling points within a square and determining how many fall inside a quarter circle inscribed within that square.\n",
    "\n",
    "![PI](https://www.101computing.net/wp/wp-content/uploads/estimating-pi-monte-carlo-method.png)\n",
    "\n",
    "The ratio between the area of the circle and the square is\n",
    "\n",
    "$\\frac{N_\\text{in}}{N_\\text{total}} = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4}$\n",
    "\n",
    "Therefore, we can calculate $\\pi$ using \n",
    "$\\pi = \\frac{4N_\\text{in}}{N_\\text{total}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30e58d-06dc-4f5a-849b-5368ea5def36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "fig, ax = plt.subplots()\n",
    "#ax = fig.add_subplot(111)\n",
    "circle = plt.Circle(( 0. , 0. ), 0.5 )\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.ylim(-0.5, 0.5)\n",
    "ax.add_patch(circle)\n",
    "ax.set_aspect('equal')\n",
    "N = 500\n",
    "Nin = 0\n",
    "t0 = time.time()\n",
    "for i in range(1, N+1):\n",
    "    x = random.uniform(-0.5, 0.5)\n",
    "    y = random.uniform(-0.5, 0.5)\n",
    "    if (np.sqrt(x*x + y*y) < 0.5):\n",
    "        Nin += 1\n",
    "        plt.plot([x], [y], 'o', color='r', markersize=3)\n",
    "    else:\n",
    "        plt.plot([x], [y], 'o', color='b', markersize=3)\n",
    "    display(fig)\n",
    "    plt.xlabel(\"$\\pi$ = %3.4f \\n N_in / N_total = %5d/%5d\" %(Nin*4.0/i, Nin, i))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "res = np.array(Nin, dtype='d')\n",
    "t1 = time.time()\n",
    "print(f\"Pi = {res/float(N/4.0)}\")\n",
    "print(\"Time: %s\" %(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7cdf58-3a6c-4fbc-b13d-ff49d792c1b0",
   "metadata": {},
   "source": [
    "### MPI example\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "N = 5000000\n",
    "Nin = 0\n",
    "t0 = time.time()\n",
    "for i in range(comm.rank, N, comm.size):\n",
    "    x = random.uniform(-0.5, 0.5)\n",
    "    y = random.uniform(-0.5, 0.5)\n",
    "    if (np.sqrt(x*x + y*y) < 0.5):\n",
    "        Nin += 1\n",
    "res = np.array(Nin, dtype='d')\n",
    "res_tot = np.array(Nin, dtype='d')\n",
    "comm.Allreduce(res, res_tot, op=MPI.SUM)\n",
    "t1 = time.time()\n",
    "if comm.rank==0:\n",
    "    print(res_tot/float(N/4.0))\n",
    "    print(\"Time: %s\" %(t1 - t0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9b141-4e85-4967-a755-69a26aaec93e",
   "metadata": {},
   "source": [
    "### Running $\\pi$ example on Google Colab\n",
    "* Go to https://colab.research.google.com/, sign in or sign up\n",
    "* \"File\"-> \"open notebook\"\n",
    "* choose ```01_intro_AI_on_Supercomputer/00_mpi.ipynb``` from the list\n",
    "![Google Colab](figures/colab.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f1404-f845-4be1-9115-37075aaf57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/01_intro_AI_on_Supercomputer/mpi_pi.py\n",
    "! pip install mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78190130-c0ff-4e33-bc97-b89c30a1cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mpirun -np 1 --allow-run-as-root python mpi_pi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb44f15-6fec-4ce6-9ea5-f923948b8dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mpirun -np 2 --allow-run-as-root --oversubscribe python mpi_pi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ec3a7-eaf2-4747-a4a3-57038fc3ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mpirun -np 4 --allow-run-as-root --oversubscribe python mpi_pi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e265058-19a7-422b-a8e1-6e938ec002f1",
   "metadata": {},
   "source": [
    "### Running $\\pi$ on Polaris\n",
    "```bash\n",
    "ssh <username>@polaris.alcf.anl.gov\n",
    "qsub -A ALCFAITP -l select=1 -q ALCFAITP -l walltime=0:30:00 -l filesystems=home:eagle\n",
    "# choose debug queue outside of the class\n",
    "# qsub -A ALCFAITP -l select=1 -q debug -l walltime=0:30:00 -l filesystems=home:eagle\n",
    "\n",
    "module load conda/2023-10-04\n",
    "conda activate /soft/datascience/ALCFAITP/2023-10-04\n",
    "git clone git@github.com:argonne-lcf/ai-science-training-series.git\n",
    "cd ai-science-training-series/01_intro_AI_on_Supercomputer/\n",
    "mpirun -np 1 python mpi_pi.py   # 3.141988,   8.029037714004517  s\n",
    "mpirun -np 2 python mpi_pi.py   # 3.1415096   4.212774038314819  s\n",
    "mpirun -np 4 python mpi_pi.py   # 3.1425632   2.093632459640503  s\n",
    "mpirun -np 8 python mpi_pi.py   # 3.1411632   1.0610620975494385 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e513ce",
   "metadata": {},
   "source": [
    "**Attention: Please use ```debug``` queue outside of lecture time instead of ```ALCFAITP```.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1351a7-50e9-4152-82bd-4213e83e2eca",
   "metadata": {},
   "source": [
    "## Parallel computing in AI\n",
    "The parallel computing in AI is usually called distributed training. Distributed training is the process of training I models across multiple GPUs or other accelerators, with the goal of speeding up the training process and enabling the training of larger models on larger datasets.\n",
    "\n",
    "There are two ways of parallelization in distributed training. \n",
    "* **Data parallelism**: \n",
    "    * Each worker (GPU) has a complete set of model\n",
    "    * different workers work on different subsets of data. \n",
    "* **Model parallelism** \n",
    "    * The model is splitted into different parts and stored on different workers\n",
    "    * Different workers work on computation involved in different parts of the model\n",
    "![PI](figures/parallel_computing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752501ce-7472-451a-b42a-b4a286127dfa",
   "metadata": {},
   "source": [
    "![3D LLM](figures/3DLLM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572f307-6cc7-481b-8ef8-1c31cef9748d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
