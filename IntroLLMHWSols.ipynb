{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fbd3f8-1305-4952-8a0f-a03886d78549",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Uncomment below section if running on sophia jupyter notebook\n",
    "'''\n",
    "import os\n",
    "os.environ[\"HTTP_PROXY\"]=\"proxy.alcf.anl.gov:3128\"\n",
    "os.environ[\"HTTPS_PROXY\"]=\"proxy.alcf.anl.gov:3128\"\n",
    "os.environ[\"http_proxy\"]=\"proxy.alcf.anl.gov:3128\"\n",
    "os.environ[\"https_proxy\"]=\"proxy.alcf.anl.gov:3128\"\n",
    "os.environ[\"ftp_proxy\"]=\"proxy.alcf.anl.gov:3128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec403fe3-55cc-4c32-afff-dd81e2d74946",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc900399-38b4-4ca7-b4ab-478ac5d450f3",
   "metadata": {},
   "source": [
    "1. Load in a generative model using the HuggingFace pipeline and generate text using a batch of prompts.\n",
    "  * Play with generative parameters such as temperature, max_new_tokens, and the model itself and explain the effect on the legibility of the model response. Try at least 4 different parameter/model combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81035819-582e-48f3-99df-876f7df6b4af",
   "metadata": {},
   "source": [
    "GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9eddd5-0b6e-4155-83f5-b575f35a3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM, AutoConfig\n",
    "prompts = ['No, I am your',\n",
    "           'Im sorry Dave, Im afraid I',\n",
    "           'What is your favorite color?',\n",
    "          'Forget it Jake, its',\n",
    "          'You cant handle the']\n",
    "\n",
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generator(prompts, max_length=20, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b243b-f547-4547-b86e-18fc9ebedfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(prompts, max_length=10, num_return_sequences=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda28cc7-4a79-4365-a759-a695c6788345",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(prompts, max_length=10, num_return_sequences=4, temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896aab3-fd6a-4700-a1f3-1acfcac1ae4d",
   "metadata": {},
   "source": [
    "Llama 3.2 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d52e85-31bb-4478-862d-b05afce51e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "hf_token = \"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\"\n",
    "login(token=hf_token, add_to_git_credential=True)\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM, AutoConfig\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B\", device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896f1dc-8f25-4beb-aa59-a29ac7b23ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"Star Wars: No, I am your\",\n",
    "    \"Space Odyssey: Im sorry Dave, Im afraid I\",\n",
    "    \"Monty Python: What is your favorite \",\n",
    "    \"Chinatown: Forget it Jake, its\",\n",
    "    \"A few good men: You cant handle the\",\n",
    "]\n",
    "pipe(messages, max_length=20, num_return_sequences=5, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a3eca-e461-4b5e-b6a0-a167603acaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(messages, max_length=20, num_return_sequences=5, temperature=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20648a-95bb-4dbf-9bc0-5515015b4085",
   "metadata": {},
   "source": [
    "2. Load in 2 models of different parameter size (e.g. GPT2, meta-llama/Llama-2-7b-chat-hf, or distilbert/distilgpt2) and analyze the BertViz for each. How does the attention mechanisms change depending on model size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652024f-acd8-4c8c-81bc-ba37e1fe3e4b",
   "metadata": {},
   "source": [
    "GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc6378-6b0a-46ef-8846-5d6fcd8d2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8803e40-7440-4196-aab6-53c6ace3bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\n",
    "\n",
    "from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "model_name = 'openai-community/gpt2'\n",
    "input_text = \"Forget it Jake, its\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = model(inputs)  # Run model\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\n",
    "model_view(attention, tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea194f-2627-4c77-8230-f44b0e887496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\n",
    "\n",
    "from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "input_text = \"Forget it Jake, its\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = model(inputs)  # Run model\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\n",
    "model_view(attention, tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28389c55-c20d-4437-8bf4-6688dfa57a66",
   "metadata": {},
   "source": [
    "We can see that loading in the llama model drastically increases the number of attention mechanisms. Although the attention mechanisms look quite similar in a lot of the heads/layers (a lot of attention to \"<|begin_of_text|>\"), due to the large number of heads + layers, there is variability in a few heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11047aab-aa34-4f90-b26b-299e94d9d78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pytorch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
